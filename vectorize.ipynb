{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "# Everything has been split roughly into 80:20 training:testing from the datasets\n",
    "####################################################################################################\n",
    "# mammogram vectorization \n",
    "# 961 samples\n",
    "# contains '?' values for undefined/missing\n",
    "# label 0 for benign, 1 for malignant\n",
    "df = pd.read_csv('datasets/mammograms/mammographic_masses.data')\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "df = df.fillna(0)\n",
    "\n",
    "mammograms = df.values\n",
    "\n",
    "mammogram_features = mammograms[:,:-1]\n",
    "mammogram_labels = mammograms[:,-1]\n",
    "\n",
    "m_train, m_test, m_train_labels, m_test_labels = train_test_split(mammogram_features, mammogram_labels, test_size=.2, random_state=23)\n",
    "####################################################################################################\n",
    "# wisconsin breast cancer vectorization\n",
    "# 669 samples\n",
    "# contains '?' values for undefined/missing\n",
    "# label 0 for benign, 1 for malignant\n",
    "\n",
    "df1 = pd.read_csv('datasets/wisconsin_breast_cancer/breast-cancer-wisconsin.data')\n",
    "df1 = df1.apply(pd.to_numeric, errors='coerce')\n",
    "df1 = df1.fillna(0)\n",
    "wbc = df1.values\n",
    "\n",
    "wbc_features = wbc[:,1:-1]\n",
    "wbc_labels = wbc[:,-1]\n",
    "\n",
    "wbc_labels /= 2\n",
    "wbc_labels -= 1\n",
    "\n",
    "wbc_train, wbc_test, wbc_train_labels, wbc_test_labels = train_test_split(wbc_features, wbc_labels, test_size=.2, random_state=1)\n",
    "####################################################################################################\n",
    "# banknote authentication set\n",
    "# 1372 samples\n",
    "# label 0 for real, 1 for forgery\n",
    "\n",
    "df2 = pd.read_csv('datasets/banknote/data_banknote_authentication.txt')\n",
    "df2 = df2.apply(pd.to_numeric, errors='coerce')\n",
    "bn = df2.values\n",
    "\n",
    "bn_features = bn[:,:-1]\n",
    "bn_labels = bn[:,-1]\n",
    "bn_train, bn_test, bn_train_labels, bn_test_labels = train_test_split(bn_features, bn_labels, test_size=.2, random_state=10)\n",
    "\n",
    "####################################################################################################\n",
    "# covtype.binary\n",
    "# 581,012 samples\n",
    "\n",
    "cov_features, cov_labels = load_svmlight_file('datasets/covtype_binary/covtype.libsvm.binary')\n",
    "\n",
    "cov_labels = cov_labels - 1\n",
    "\n",
    "cov_train, cov_test, cov_train_labels, cov_test_labels = train_test_split(cov_features, cov_labels, test_size=.2, random_state=4)\n",
    "####################################################################################################\n",
    "# spam_sms vectorization\n",
    "# \n",
    "\n",
    "#df2 = pd.read_csv('datasets/spam_sms/SMSSpamCollection')\n",
    "\n",
    "#sms_raw = pd.read_csv(\"datasets/spam_sms/SMSSpamCollection\",)\n",
    "\n",
    "####################################################################################################\n",
    "# AUS rain prediction vectorization\n",
    "# ACHTUNG: might need more preprocessing because of text values in some\n",
    "\n",
    "#df1 = pd.read_csv(\"datasets/AUS_rain/weatherAUS.csv\")\n",
    "#df_features = df1.drop(['RISK_MM','RainTomorrow'], axis=1)\n",
    "#df_labels = df1[['RainTomorrow']]\n",
    "\n",
    "#aus_features = df_features.values\n",
    "#aus_labels = df_labels.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/willye/anaconda3/envs/py27/lib/python2.7/site-packages/ipykernel_launcher.py:28: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/willye/anaconda3/envs/py27/lib/python2.7/site-packages/ipykernel_launcher.py:28: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on epoch 0: nan\n",
      "Loss on epoch 100: nan\n",
      "Loss on epoch 200: 8.622407\n",
      "Loss on epoch 300: 1.489364\n",
      "Loss on epoch 400: 0.773710\n",
      "Loss on epoch 500: 0.772283\n",
      "Loss on epoch 600: 0.770879\n",
      "Loss on epoch 700: 0.769480\n",
      "Loss on epoch 800: 0.768088\n",
      "Loss on epoch 900: 0.766702\n",
      "Loss on epoch 1000: 0.765322\n",
      "Loss on epoch 1100: 0.763949\n",
      "Loss on epoch 1200: 0.762582\n",
      "Loss on epoch 1300: 0.761221\n",
      "Loss on epoch 1400: 0.759866\n",
      "Loss on epoch 1500: 0.758518\n",
      "Loss on epoch 1600: 0.757176\n",
      "Loss on epoch 1700: 0.755841\n",
      "Loss on epoch 1800: 0.754511\n",
      "Loss on epoch 1900: 0.753188\n",
      "Loss on epoch 2000: 0.751871\n",
      "Loss on epoch 2100: 0.750560\n",
      "Loss on epoch 2200: 0.749256\n",
      "Loss on epoch 2300: 0.747958\n",
      "Loss on epoch 2400: 0.746666\n",
      "Loss on epoch 2500: 0.745380\n",
      "Loss on epoch 2600: 0.744101\n",
      "Loss on epoch 2700: 0.742828\n",
      "Loss on epoch 2800: 0.741561\n",
      "Loss on epoch 2900: 0.740300\n",
      "Loss on epoch 3000: 0.739045\n",
      "Loss on epoch 3100: 0.737797\n",
      "Loss on epoch 3200: 0.736555\n",
      "Loss on epoch 3300: 0.735318\n",
      "Loss on epoch 3400: 0.734089\n",
      "Loss on epoch 3500: 0.732865\n",
      "Loss on epoch 3600: 0.731647\n",
      "Loss on epoch 3700: 0.730436\n",
      "Loss on epoch 3800: 0.729231\n",
      "Loss on epoch 3900: 0.728031\n",
      "Loss on epoch 4000: 0.726838\n",
      "Loss on epoch 4100: 0.725651\n",
      "Loss on epoch 4200: 0.724471\n",
      "Loss on epoch 4300: 0.723296\n",
      "Loss on epoch 4400: 0.722127\n",
      "Loss on epoch 4500: 0.720965\n",
      "Loss on epoch 4600: 0.719808\n",
      "Loss on epoch 4700: 0.718658\n",
      "Loss on epoch 4800: 0.717513\n",
      "Loss on epoch 4900: 0.716375\n",
      "Loss on epoch 5000: 0.715243\n",
      "Loss on epoch 5100: 0.714116\n",
      "Loss on epoch 5200: 0.712996\n",
      "Loss on epoch 5300: 0.711882\n",
      "Loss on epoch 5400: 0.710773\n",
      "Loss on epoch 5500: 0.709671\n",
      "Loss on epoch 5600: 0.708574\n",
      "Loss on epoch 5700: 0.707484\n",
      "Loss on epoch 5800: 0.706399\n",
      "Loss on epoch 5900: 0.705321\n",
      "Loss on epoch 6000: 0.704248\n",
      "Loss on epoch 6100: 0.703181\n",
      "Loss on epoch 6200: 0.702120\n",
      "Loss on epoch 6300: 0.701065\n",
      "Loss on epoch 6400: 0.700015\n",
      "Loss on epoch 6500: 0.698972\n",
      "Loss on epoch 6600: 0.697934\n",
      "Loss on epoch 6700: 0.696903\n",
      "Loss on epoch 6800: 0.695877\n",
      "Loss on epoch 6900: 0.694856\n",
      "Loss on epoch 7000: 0.693842\n",
      "Loss on epoch 7100: 0.692833\n",
      "Loss on epoch 7200: 0.691830\n",
      "Loss on epoch 7300: 0.690833\n",
      "Loss on epoch 7400: 0.689841\n",
      "Loss on epoch 7500: 0.688856\n",
      "Loss on epoch 7600: 0.687875\n",
      "Loss on epoch 7700: 0.686901\n",
      "Loss on epoch 7800: 0.685932\n",
      "Loss on epoch 7900: 0.684969\n",
      "Loss on epoch 8000: 0.684011\n",
      "Loss on epoch 8100: 0.683059\n",
      "Loss on epoch 8200: 0.682113\n",
      "Loss on epoch 8300: 0.681172\n",
      "Loss on epoch 8400: 0.680237\n",
      "Loss on epoch 8500: 0.679307\n",
      "Loss on epoch 8600: 0.678382\n",
      "Loss on epoch 8700: 0.677464\n",
      "Loss on epoch 8800: 0.676550\n",
      "Loss on epoch 8900: 0.675643\n",
      "Loss on epoch 9000: 0.674740\n",
      "Loss on epoch 9100: 0.673843\n",
      "Loss on epoch 9200: 0.672952\n",
      "Loss on epoch 9300: 0.672066\n",
      "Loss on epoch 9400: 0.671185\n",
      "Loss on epoch 9500: 0.670310\n",
      "Loss on epoch 9600: 0.669440\n",
      "Loss on epoch 9700: 0.668575\n",
      "Loss on epoch 9800: 0.667716\n",
      "Loss on epoch 9900: 0.666862\n",
      "Loss on epoch 10000: 0.666013\n",
      "Loss on epoch 10100: 0.665169\n",
      "Loss on epoch 10200: 0.664331\n",
      "Loss on epoch 10300: 0.663498\n",
      "Loss on epoch 10400: 0.662670\n",
      "Loss on epoch 10500: 0.661847\n",
      "Loss on epoch 10600: 0.661030\n",
      "Loss on epoch 10700: 0.660217\n",
      "Loss on epoch 10800: 0.659410\n",
      "Loss on epoch 10900: 0.658608\n",
      "Loss on epoch 11000: 0.657811\n",
      "Loss on epoch 11100: 0.657019\n",
      "Loss on epoch 11200: 0.656232\n",
      "Loss on epoch 11300: 0.655450\n",
      "Loss on epoch 11400: 0.654673\n",
      "Loss on epoch 11500: 0.653902\n",
      "Loss on epoch 11600: 0.653135\n",
      "Loss on epoch 11700: 0.652373\n",
      "Loss on epoch 11800: 0.651616\n",
      "Loss on epoch 11900: 0.650864\n",
      "Loss on epoch 12000: 0.650117\n",
      "Loss on epoch 12100: 0.649375\n",
      "Loss on epoch 12200: 0.648638\n",
      "Loss on epoch 12300: 0.647905\n",
      "Loss on epoch 12400: 0.647178\n",
      "Loss on epoch 12500: 0.646455\n",
      "Loss on epoch 12600: 0.645738\n",
      "Loss on epoch 12700: 0.645025\n",
      "Loss on epoch 12800: 0.644317\n",
      "Loss on epoch 12900: 0.643613\n",
      "Loss on epoch 13000: 0.642915\n",
      "Loss on epoch 13100: 0.642221\n",
      "Loss on epoch 13200: 0.641532\n",
      "Loss on epoch 13300: 0.640848\n",
      "Loss on epoch 13400: 0.640169\n",
      "Loss on epoch 13500: 0.639494\n",
      "Loss on epoch 13600: 0.638824\n",
      "Loss on epoch 13700: 0.638159\n",
      "Loss on epoch 13800: 0.637498\n",
      "Loss on epoch 13900: 0.636842\n",
      "Loss on epoch 14000: 0.636191\n",
      "Loss on epoch 14100: 0.635545\n",
      "Loss on epoch 14200: 0.634904\n",
      "Loss on epoch 14300: 0.634267\n",
      "Loss on epoch 14400: 0.633635\n",
      "Loss on epoch 14500: 0.633007\n",
      "Loss on epoch 14600: 0.632385\n",
      "Loss on epoch 14700: 0.631767\n",
      "Loss on epoch 14800: 0.631154\n",
      "Loss on epoch 14900: 0.630545\n",
      "Loss on epoch 15000: 0.629942\n",
      "Loss on epoch 15100: 0.629343\n",
      "Loss on epoch 15200: 0.628749\n",
      "Loss on epoch 15300: 0.628160\n",
      "Loss on epoch 15400: 0.627576\n",
      "Loss on epoch 15500: 0.626997\n",
      "Loss on epoch 15600: 0.626422\n",
      "Loss on epoch 15700: 0.625853\n",
      "Loss on epoch 15800: 0.625288\n",
      "Loss on epoch 15900: 0.624729\n",
      "Loss on epoch 16000: 0.624174\n",
      "Loss on epoch 16100: 0.623625\n",
      "Loss on epoch 16200: 0.623080\n",
      "Loss on epoch 16300: 0.622541\n",
      "Loss on epoch 16400: 0.622007\n",
      "Loss on epoch 16500: 0.621478\n",
      "Loss on epoch 16600: 0.620954\n",
      "Loss on epoch 16700: 0.620435\n",
      "Loss on epoch 16800: 0.619922\n",
      "Loss on epoch 16900: 0.619414\n",
      "Loss on epoch 17000: 0.618911\n",
      "Loss on epoch 17100: 0.618414\n",
      "Loss on epoch 17200: 0.617921\n",
      "Loss on epoch 17300: 0.617435\n",
      "Loss on epoch 17400: 0.616953\n",
      "Loss on epoch 17500: 0.616477\n",
      "Loss on epoch 17600: 0.616006\n",
      "Loss on epoch 17700: 0.615541\n",
      "Loss on epoch 17800: 0.615081\n",
      "Loss on epoch 17900: 0.614626\n",
      "Loss on epoch 18000: 0.614177\n",
      "Loss on epoch 18100: 0.613733\n",
      "Loss on epoch 18200: 0.613295\n",
      "Loss on epoch 18300: 0.612861\n",
      "Loss on epoch 18400: 0.612433\n",
      "Loss on epoch 18500: 0.612010\n",
      "Loss on epoch 18600: 0.611592\n",
      "Loss on epoch 18700: 0.611180\n",
      "Loss on epoch 18800: 0.610772\n",
      "Loss on epoch 18900: 0.610369\n",
      "Loss on epoch 19000: 0.609972\n",
      "Loss on epoch 19100: 0.609579\n",
      "Loss on epoch 19200: 0.609191\n",
      "Loss on epoch 19300: 0.608807\n",
      "Loss on epoch 19400: 0.608429\n",
      "Loss on epoch 19500: 0.608055\n",
      "Loss on epoch 19600: 0.607685\n",
      "Loss on epoch 19700: 0.607320\n",
      "Loss on epoch 19800: 0.606959\n",
      "Loss on epoch 19900: 0.606602\n",
      "Loss on epoch 20000: 0.606250\n",
      "Loss on epoch 20100: 0.605901\n",
      "Loss on epoch 20200: 0.605557\n",
      "Loss on epoch 20300: 0.605217\n",
      "Loss on epoch 20400: 0.604880\n",
      "Loss on epoch 20500: 0.604547\n",
      "Loss on epoch 20600: 0.604218\n",
      "Loss on epoch 20700: 0.603893\n",
      "Loss on epoch 20800: 0.603571\n",
      "Loss on epoch 20900: 0.603253\n",
      "Loss on epoch 21000: 0.602938\n",
      "Loss on epoch 21100: 0.602626\n",
      "Loss on epoch 21200: 0.602317\n",
      "Loss on epoch 21300: 0.602012\n",
      "Loss on epoch 21400: 0.601710\n",
      "Loss on epoch 21500: 0.601411\n",
      "Loss on epoch 21600: 0.601115\n",
      "Loss on epoch 21700: 0.600822\n",
      "Loss on epoch 21800: 0.600532\n",
      "Loss on epoch 21900: 0.600244\n",
      "Loss on epoch 22000: 0.599960\n",
      "Loss on epoch 22100: 0.599678\n",
      "Loss on epoch 22200: 0.599398\n",
      "Loss on epoch 22300: 0.599122\n",
      "Loss on epoch 22400: 0.598848\n",
      "Loss on epoch 22500: 0.598576\n",
      "Loss on epoch 22600: 0.598307\n",
      "Loss on epoch 22700: 0.598040\n",
      "Loss on epoch 22800: 0.597776\n",
      "Loss on epoch 22900: 0.597514\n",
      "Loss on epoch 23000: 0.597254\n",
      "Loss on epoch 23100: 0.596997\n",
      "Loss on epoch 23200: 0.596742\n",
      "Loss on epoch 23300: 0.596489\n",
      "Loss on epoch 23400: 0.596238\n",
      "Loss on epoch 23500: 0.595989\n",
      "Loss on epoch 23600: 0.595743\n",
      "Loss on epoch 23700: 0.595498\n",
      "Loss on epoch 23800: 0.595255\n",
      "Loss on epoch 23900: 0.595015\n",
      "Loss on epoch 24000: 0.594776\n",
      "Loss on epoch 24100: 0.594540\n",
      "Loss on epoch 24200: 0.594305\n",
      "Loss on epoch 24300: 0.594072\n",
      "Loss on epoch 24400: 0.593841\n",
      "Loss on epoch 24500: 0.593611\n",
      "Loss on epoch 24600: 0.593384\n",
      "Loss on epoch 24700: 0.593158\n",
      "Loss on epoch 24800: 0.592934\n",
      "Loss on epoch 24900: 0.592712\n",
      "Loss on epoch 25000: 0.592492\n",
      "Loss on epoch 25100: 0.592273\n",
      "Loss on epoch 25200: 0.592056\n",
      "Loss on epoch 25300: 0.591840\n",
      "Loss on epoch 25400: 0.591626\n",
      "Loss on epoch 25500: 0.591414\n",
      "Loss on epoch 25600: 0.591203\n",
      "Loss on epoch 25700: 0.590994\n",
      "Loss on epoch 25800: 0.590786\n",
      "Loss on epoch 25900: 0.590580\n",
      "Loss on epoch 26000: 0.590376\n",
      "Loss on epoch 26100: 0.590172\n",
      "Loss on epoch 26200: 0.589971\n",
      "Loss on epoch 26300: 0.589771\n",
      "Loss on epoch 26400: 0.589572\n",
      "Loss on epoch 26500: 0.589375\n",
      "Loss on epoch 26600: 0.589179\n",
      "Loss on epoch 26700: 0.588984\n",
      "Loss on epoch 26800: 0.588791\n",
      "Loss on epoch 26900: 0.588599\n",
      "Loss on epoch 27000: 0.588409\n",
      "Loss on epoch 27100: 0.588220\n",
      "Loss on epoch 27200: 0.588032\n",
      "Loss on epoch 27300: 0.587846\n",
      "Loss on epoch 27400: 0.587660\n",
      "Loss on epoch 27500: 0.587477\n",
      "Loss on epoch 27600: 0.587294\n",
      "Loss on epoch 27700: 0.587113\n",
      "Loss on epoch 27800: 0.586933\n",
      "Loss on epoch 27900: 0.586754\n",
      "Loss on epoch 28000: 0.586576\n",
      "Loss on epoch 28100: 0.586400\n",
      "Loss on epoch 28200: 0.586225\n",
      "Loss on epoch 28300: 0.586051\n",
      "Loss on epoch 28400: 0.585878\n",
      "Loss on epoch 28500: 0.585706\n",
      "Loss on epoch 28600: 0.585536\n",
      "Loss on epoch 28700: 0.585366\n",
      "Loss on epoch 28800: 0.585198\n",
      "Loss on epoch 28900: 0.585031\n",
      "Loss on epoch 29000: 0.584865\n",
      "Loss on epoch 29100: 0.584700\n",
      "Loss on epoch 29200: 0.584536\n",
      "Loss on epoch 29300: 0.584373\n",
      "Loss on epoch 29400: 0.584212\n",
      "Loss on epoch 29500: 0.584051\n",
      "Loss on epoch 29600: 0.583892\n",
      "Loss on epoch 29700: 0.583733\n",
      "Loss on epoch 29800: 0.583576\n",
      "Loss on epoch 29900: 0.583419\n",
      "Loss on epoch 30000: 0.583264\n",
      "Loss on epoch 30100: 0.583110\n",
      "Loss on epoch 30200: 0.582956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on epoch 30300: 0.582804\n",
      "Loss on epoch 30400: 0.582652\n",
      "Loss on epoch 30500: 0.582502\n",
      "Loss on epoch 30600: 0.582353\n",
      "Loss on epoch 30700: 0.582204\n",
      "Loss on epoch 30800: 0.582057\n",
      "Loss on epoch 30900: 0.581910\n",
      "Loss on epoch 31000: 0.581764\n",
      "Loss on epoch 31100: 0.581620\n",
      "Loss on epoch 31200: 0.581476\n",
      "Loss on epoch 31300: 0.581333\n",
      "Loss on epoch 31400: 0.581191\n",
      "Loss on epoch 31500: 0.581050\n",
      "Loss on epoch 31600: 0.580910\n",
      "Loss on epoch 31700: 0.580770\n",
      "Loss on epoch 31800: 0.580632\n",
      "Loss on epoch 31900: 0.580494\n",
      "Loss on epoch 32000: 0.580358\n",
      "Loss on epoch 32100: 0.580222\n",
      "Loss on epoch 32200: 0.580087\n",
      "Loss on epoch 32300: 0.579953\n",
      "Loss on epoch 32400: 0.579819\n",
      "Loss on epoch 32500: 0.579687\n",
      "Loss on epoch 32600: 0.579555\n",
      "Loss on epoch 32700: 0.579424\n",
      "Loss on epoch 32800: 0.579294\n",
      "Loss on epoch 32900: 0.579165\n",
      "Loss on epoch 33000: 0.579036\n",
      "Loss on epoch 33100: 0.578909\n",
      "Loss on epoch 33200: 0.578782\n",
      "Loss on epoch 33300: 0.578656\n",
      "Loss on epoch 33400: 0.578530\n",
      "Loss on epoch 33500: 0.578406\n",
      "Loss on epoch 33600: 0.578282\n",
      "Loss on epoch 33700: 0.578159\n",
      "Loss on epoch 33800: 0.578037\n",
      "Loss on epoch 33900: 0.577915\n",
      "Loss on epoch 34000: 0.577794\n",
      "Loss on epoch 34100: 0.577674\n",
      "Loss on epoch 34200: 0.577555\n",
      "Loss on epoch 34300: 0.577436\n",
      "Loss on epoch 34400: 0.577318\n",
      "Loss on epoch 34500: 0.577201\n",
      "Loss on epoch 34600: 0.577084\n",
      "Loss on epoch 34700: 0.576968\n",
      "Loss on epoch 34800: 0.576853\n",
      "Loss on epoch 34900: 0.576739\n",
      "Loss on epoch 35000: 0.576625\n",
      "Loss on epoch 35100: 0.576512\n",
      "Loss on epoch 35200: 0.576399\n",
      "Loss on epoch 35300: 0.576288\n",
      "Loss on epoch 35400: 0.576176\n",
      "Loss on epoch 35500: 0.576066\n",
      "Loss on epoch 35600: 0.575956\n",
      "Loss on epoch 35700: 0.575847\n",
      "Loss on epoch 35800: 0.575738\n",
      "Loss on epoch 35900: 0.575630\n",
      "Loss on epoch 36000: 0.575523\n",
      "Loss on epoch 36100: 0.575416\n",
      "Loss on epoch 36200: 0.575310\n",
      "Loss on epoch 36300: 0.575205\n",
      "Loss on epoch 36400: 0.575100\n",
      "Loss on epoch 36500: 0.574996\n",
      "Loss on epoch 36600: 0.574892\n",
      "Loss on epoch 36700: 0.574789\n",
      "Loss on epoch 36800: 0.574687\n",
      "Loss on epoch 36900: 0.574585\n",
      "Loss on epoch 37000: 0.574484\n",
      "Loss on epoch 37100: 0.574383\n",
      "Loss on epoch 37200: 0.574283\n",
      "Loss on epoch 37300: 0.574183\n",
      "Loss on epoch 37400: 0.574084\n",
      "Loss on epoch 37500: 0.573986\n",
      "Loss on epoch 37600: 0.573888\n",
      "Loss on epoch 37700: 0.573791\n",
      "Loss on epoch 37800: 0.573694\n",
      "Loss on epoch 37900: 0.573598\n",
      "Loss on epoch 38000: 0.573502\n",
      "Loss on epoch 38100: 0.573407\n",
      "Loss on epoch 38200: 0.573313\n",
      "Loss on epoch 38300: 0.573219\n",
      "Loss on epoch 38400: 0.573125\n",
      "Loss on epoch 38500: 0.573032\n",
      "Loss on epoch 38600: 0.572940\n",
      "Loss on epoch 38700: 0.572848\n",
      "Loss on epoch 38800: 0.572756\n",
      "Loss on epoch 38900: 0.572665\n",
      "Loss on epoch 39000: 0.572575\n",
      "Loss on epoch 39100: 0.572485\n",
      "Loss on epoch 39200: 0.572395\n",
      "Loss on epoch 39300: 0.572306\n",
      "Loss on epoch 39400: 0.572218\n",
      "Loss on epoch 39500: 0.572130\n",
      "Loss on epoch 39600: 0.572042\n",
      "Loss on epoch 39700: 0.571955\n",
      "Loss on epoch 39800: 0.571869\n",
      "Loss on epoch 39900: 0.571783\n",
      "139\n",
      "192\n",
      "0.723958333333\n",
      "Loss on epoch 0: 3.552337\n",
      "Loss on epoch 100: 0.466680\n",
      "Loss on epoch 200: 0.416697\n",
      "Loss on epoch 300: 0.402304\n",
      "Loss on epoch 400: 0.396834\n",
      "Loss on epoch 500: 0.394261\n",
      "Loss on epoch 600: 0.392820\n",
      "Loss on epoch 700: 0.391896\n",
      "Loss on epoch 800: 0.391245\n",
      "Loss on epoch 900: 0.390758\n",
      "Loss on epoch 1000: 0.390378\n",
      "Loss on epoch 1100: 0.390075\n",
      "Loss on epoch 1200: 0.389828\n",
      "Loss on epoch 1300: 0.389626\n",
      "Loss on epoch 1400: 0.389458\n",
      "Loss on epoch 1500: 0.389318\n",
      "Loss on epoch 1600: 0.389200\n",
      "Loss on epoch 1700: 0.389101\n",
      "Loss on epoch 1800: 0.389017\n",
      "Loss on epoch 1900: 0.388945\n",
      "Loss on epoch 2000: 0.388884\n",
      "Loss on epoch 2100: 0.388831\n",
      "Loss on epoch 2200: 0.388786\n",
      "Loss on epoch 2300: 0.388748\n",
      "Loss on epoch 2400: 0.388714\n",
      "Loss on epoch 2500: 0.388685\n",
      "Loss on epoch 2600: 0.388660\n",
      "Loss on epoch 2700: 0.388639\n",
      "Loss on epoch 2800: 0.388620\n",
      "Loss on epoch 2900: 0.388604\n",
      "Loss on epoch 3000: 0.388589\n",
      "Loss on epoch 3100: 0.388577\n",
      "Loss on epoch 3200: 0.388566\n",
      "Loss on epoch 3300: 0.388557\n",
      "Loss on epoch 3400: 0.388549\n",
      "Loss on epoch 3500: 0.388542\n",
      "Loss on epoch 3600: 0.388535\n",
      "Loss on epoch 3700: 0.388530\n",
      "Loss on epoch 3800: 0.388525\n",
      "Loss on epoch 3900: 0.388521\n",
      "Loss on epoch 4000: 0.388517\n",
      "Loss on epoch 4100: 0.388514\n",
      "Loss on epoch 4200: 0.388511\n",
      "Loss on epoch 4300: 0.388509\n",
      "Loss on epoch 4400: 0.388507\n",
      "Loss on epoch 4500: 0.388505\n",
      "Loss on epoch 4600: 0.388503\n",
      "Loss on epoch 4700: 0.388502\n",
      "Loss on epoch 4800: 0.388500\n",
      "Loss on epoch 4900: 0.388499\n",
      "126\n",
      "140\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "num_samples, num_features = m_train.shape        \n",
    "\n",
    "gd_w = np.random.uniform(size=num_features)\n",
    "\n",
    "gd_loss_history = []\n",
    "\n",
    "def gradient_descent_square_error(X,y,epochs=50000,lr=0.001):\n",
    "    num_samples, num_features = X.shape\n",
    "    \n",
    "    for i in xrange(epochs):\n",
    "        global gd_w\n",
    "        predictions = sigmoid(np.dot(X,gd_w))\n",
    "        loss=np.sum((predictions-y)**2)\n",
    "        gd_loss_history.append(loss)\n",
    "    \n",
    "        print(\"[INFO] epoch #{}, loss={:.7f}\".format(i + 1, loss))\n",
    "        \n",
    "        gradient=np.dot(X.T,predictions-y) / num_samples\n",
    "        gd_w+= -gradient * lr\n",
    "\n",
    "def log_loss(X,y):\n",
    "    global gd_w\n",
    "    s = np.dot(X,gd_w)\n",
    "    predictions=sigmoid(s)\n",
    "    \n",
    "    log_l = (-y*np.log(predictions)-(1-y)*np.log(1-predictions)).mean()\n",
    "    return log_l\n",
    "    \n",
    "    \n",
    "def gradient_descent_logistic_reg(X,y,epochs=5000,lr=0.01):\n",
    "    num_samples, num_features = X.shape\n",
    "    \n",
    "    for i in xrange(epochs):\n",
    "        global gd_w\n",
    "        s = np.dot(X,gd_w)\n",
    "        predictions = sigmoid(s)\n",
    "        error = predictions-y\n",
    "        gradient=np.dot(X.T,error)/num_samples\n",
    "        \n",
    "        gd_w += -gradient * lr\n",
    "        if i % 100 == 0:\n",
    "            print \"Loss on epoch %d: %f\"%(i,log_loss(X,y))\n",
    "#             print gd_w\n",
    "    \n",
    "    \n",
    "def check_accuracy(X,y):\n",
    "    num_samples, num_features = X.shape\n",
    "    accurate = 0\n",
    "    global gd_w\n",
    "    \n",
    "    for i in xrange(num_samples):\n",
    "        prediction = sigmoid(np.dot(X[i],gd_w))\n",
    "        label = 0\n",
    "        \n",
    "        if prediction >= .5:\n",
    "            label = 1\n",
    "        \n",
    "       # print label\n",
    "        #print y[i]\n",
    "        if label == y[i]:\n",
    "            accurate += 1\n",
    "    print accurate\n",
    "    print num_samples\n",
    "    return accurate / float(num_samples)\n",
    "\n",
    "# wipe gd_w\n",
    "gradient_descent_logistic_reg(m_train, m_train_labels,40000,0.0001)\n",
    "print (check_accuracy(m_test, m_test_labels))\n",
    "\n",
    "num_samples, num_features = wbc_train.shape        \n",
    "\n",
    "gd_w = np.random.uniform(size=num_features)\n",
    "gradient_descent_logistic_reg(wbc_train, wbc_train_labels)\n",
    "print (check_accuracy(wbc_test, wbc_test_labels))\n",
    "\n",
    "# num_samples, num_features = cov_train.shape        \n",
    "\n",
    "# gd_w = np.random.uniform(size=num_features)\n",
    "# gradient_descent_logistic_reg(cov_train.toarray(), cov_train_labels,1000,0.1)\n",
    "# print (check_accuracy(cov_test.toarray(), cov_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
