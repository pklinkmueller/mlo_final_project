{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "# Everything has been split roughly into 80:20 training:testing from the datasets\n",
    "####################################################################################################\n",
    "# mammogram vectorization \n",
    "# 961 samples\n",
    "# contains '?' values for undefined/missing\n",
    "# label 0 for benign, 1 for malignant\n",
    "df = pd.read_csv('datasets/mammograms/mammographic_masses.data')\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "df = df.fillna(0)\n",
    "\n",
    "mammograms = df.values\n",
    "\n",
    "mammogram_features = mammograms[:,:-1]\n",
    "mammogram_labels = mammograms[:,-1]\n",
    "\n",
    "m_train, m_test, m_train_labels, m_test_labels = train_test_split(mammogram_features, mammogram_labels, test_size=.2, random_state=23)\n",
    "####################################################################################################\n",
    "# wisconsin breast cancer vectorization\n",
    "# 669 samples\n",
    "# contains '?' values for undefined/missing\n",
    "# label 0 for benign, 1 for malignant\n",
    "\n",
    "df1 = pd.read_csv('datasets/wisconsin_breast_cancer/breast-cancer-wisconsin.data')\n",
    "df1 = df1.apply(pd.to_numeric, errors='coerce')\n",
    "df1 = df1.fillna(0)\n",
    "wbc = df1.values\n",
    "\n",
    "wbc_features = wbc[:,1:-1]\n",
    "wbc_labels = wbc[:,-1]\n",
    "\n",
    "wbc_labels /= 2\n",
    "wbc_labels -= 1\n",
    "\n",
    "wbc_train, wbc_test, wbc_train_labels, wbc_test_labels = train_test_split(wbc_features, wbc_labels, test_size=.2, random_state=1)\n",
    "####################################################################################################\n",
    "# banknote authentication set\n",
    "# 1372 samples\n",
    "# label 0 for real, 1 for forgery\n",
    "\n",
    "df2 = pd.read_csv('datasets/banknote/data_banknote_authentication.txt')\n",
    "df2 = df2.apply(pd.to_numeric, errors='coerce')\n",
    "bn = df2.values\n",
    "\n",
    "bn_features = bn[:,:-1]\n",
    "bn_labels = bn[:,-1]\n",
    "bn_train, bn_test, bn_train_labels, bn_test_labels = train_test_split(bn_features, bn_labels, test_size=.2, random_state=10)\n",
    "\n",
    "####################################################################################################\n",
    "# news20 test\n",
    "\n",
    "news_features, news_labels = load_svmlight_file('datasets/news20/news20.binary')\n",
    "\n",
    "news_labels += 1\n",
    "news_labels /= 2\n",
    "\n",
    "news_train, news_test, news_train_labels, news_test_labels = train_test_split(news_features, news_labels, test_size = .2, random_state=10)\n",
    "\n",
    "####################################################################################################\n",
    "# cod-rna\n",
    "cod_features, cod_labels = load_svmlight_file('datasets/cod_rna/cod-rna')\n",
    "\n",
    "cod_labels += 1\n",
    "cod_labels /= 2\n",
    "\n",
    "cod_train,cod_test, cod_train_labels, cod_test_labels = train_test_split(cod_features, cod_labels, test_size = .2, random_state=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/willye/anaconda3/envs/py27/lib/python2.7/site-packages/ipykernel_launcher.py:28: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/willye/anaconda3/envs/py27/lib/python2.7/site-packages/ipykernel_launcher.py:28: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on epoch 0: nan\n",
      "Loss on epoch 100: 8.868913\n",
      "Loss on epoch 200: 1.666953\n",
      "Loss on epoch 300: 0.709468\n",
      "Loss on epoch 400: 0.708417\n",
      "Loss on epoch 500: 0.707406\n",
      "Loss on epoch 600: 0.706401\n",
      "Loss on epoch 700: 0.705403\n",
      "Loss on epoch 800: 0.704411\n",
      "Loss on epoch 900: 0.703425\n",
      "Loss on epoch 1000: 0.702447\n",
      "Loss on epoch 1100: 0.701475\n",
      "Loss on epoch 1200: 0.700510\n",
      "Loss on epoch 1300: 0.699552\n",
      "Loss on epoch 1400: 0.698601\n",
      "Loss on epoch 1500: 0.697657\n",
      "Loss on epoch 1600: 0.696720\n",
      "Loss on epoch 1700: 0.695791\n",
      "Loss on epoch 1800: 0.694869\n",
      "Loss on epoch 1900: 0.693954\n",
      "Loss on epoch 2000: 0.693047\n",
      "Loss on epoch 2100: 0.692148\n",
      "Loss on epoch 2200: 0.691257\n",
      "Loss on epoch 2300: 0.690374\n",
      "Loss on epoch 2400: 0.689499\n",
      "Loss on epoch 2500: 0.688631\n",
      "Loss on epoch 2600: 0.687772\n",
      "Loss on epoch 2700: 0.686921\n",
      "Loss on epoch 2800: 0.686078\n",
      "Loss on epoch 2900: 0.685243\n",
      "Loss on epoch 3000: 0.684417\n",
      "Loss on epoch 3100: 0.683598\n",
      "Loss on epoch 3200: 0.682788\n",
      "Loss on epoch 3300: 0.681986\n",
      "Loss on epoch 3400: 0.681191\n",
      "Loss on epoch 3500: 0.680405\n",
      "Loss on epoch 3600: 0.679627\n",
      "Loss on epoch 3700: 0.678856\n",
      "Loss on epoch 3800: 0.678093\n",
      "Loss on epoch 3900: 0.677338\n",
      "Loss on epoch 4000: 0.676590\n",
      "Loss on epoch 4100: 0.675850\n",
      "Loss on epoch 4200: 0.675116\n",
      "Loss on epoch 4300: 0.674390\n",
      "Loss on epoch 4400: 0.673671\n",
      "Loss on epoch 4500: 0.672959\n",
      "Loss on epoch 4600: 0.672254\n",
      "Loss on epoch 4700: 0.671555\n",
      "Loss on epoch 4800: 0.670862\n",
      "Loss on epoch 4900: 0.670176\n",
      "Loss on epoch 5000: 0.669496\n",
      "Loss on epoch 5100: 0.668822\n",
      "Loss on epoch 5200: 0.668153\n",
      "Loss on epoch 5300: 0.667491\n",
      "Loss on epoch 5400: 0.666834\n",
      "Loss on epoch 5500: 0.666183\n",
      "Loss on epoch 5600: 0.665537\n",
      "Loss on epoch 5700: 0.664896\n",
      "Loss on epoch 5800: 0.664261\n",
      "Loss on epoch 5900: 0.663631\n",
      "Loss on epoch 6000: 0.663005\n",
      "Loss on epoch 6100: 0.662385\n",
      "Loss on epoch 6200: 0.661769\n",
      "Loss on epoch 6300: 0.661158\n",
      "Loss on epoch 6400: 0.660551\n",
      "Loss on epoch 6500: 0.659949\n",
      "Loss on epoch 6600: 0.659351\n",
      "Loss on epoch 6700: 0.658758\n",
      "Loss on epoch 6800: 0.658169\n",
      "Loss on epoch 6900: 0.657584\n",
      "Loss on epoch 7000: 0.657003\n",
      "Loss on epoch 7100: 0.656427\n",
      "Loss on epoch 7200: 0.655854\n",
      "Loss on epoch 7300: 0.655285\n",
      "Loss on epoch 7400: 0.654720\n",
      "Loss on epoch 7500: 0.654159\n",
      "Loss on epoch 7600: 0.653602\n",
      "Loss on epoch 7700: 0.653048\n",
      "Loss on epoch 7800: 0.652499\n",
      "Loss on epoch 7900: 0.651952\n",
      "Loss on epoch 8000: 0.651410\n",
      "Loss on epoch 8100: 0.650870\n",
      "Loss on epoch 8200: 0.650335\n",
      "Loss on epoch 8300: 0.649802\n",
      "Loss on epoch 8400: 0.649274\n",
      "Loss on epoch 8500: 0.648748\n",
      "Loss on epoch 8600: 0.648226\n",
      "Loss on epoch 8700: 0.647707\n",
      "Loss on epoch 8800: 0.647192\n",
      "Loss on epoch 8900: 0.646679\n",
      "Loss on epoch 9000: 0.646170\n",
      "Loss on epoch 9100: 0.645664\n",
      "Loss on epoch 9200: 0.645162\n",
      "Loss on epoch 9300: 0.644662\n",
      "Loss on epoch 9400: 0.644166\n",
      "Loss on epoch 9500: 0.643672\n",
      "Loss on epoch 9600: 0.643182\n",
      "Loss on epoch 9700: 0.642694\n",
      "Loss on epoch 9800: 0.642210\n",
      "Loss on epoch 9900: 0.641729\n",
      "Loss on epoch 10000: 0.641250\n",
      "Loss on epoch 10100: 0.640774\n",
      "Loss on epoch 10200: 0.640302\n",
      "Loss on epoch 10300: 0.639832\n",
      "Loss on epoch 10400: 0.639365\n",
      "Loss on epoch 10500: 0.638901\n",
      "Loss on epoch 10600: 0.638440\n",
      "Loss on epoch 10700: 0.637981\n",
      "Loss on epoch 10800: 0.637526\n",
      "Loss on epoch 10900: 0.637073\n",
      "Loss on epoch 11000: 0.636623\n",
      "Loss on epoch 11100: 0.636175\n",
      "Loss on epoch 11200: 0.635730\n",
      "Loss on epoch 11300: 0.635288\n",
      "Loss on epoch 11400: 0.634849\n",
      "Loss on epoch 11500: 0.634412\n",
      "Loss on epoch 11600: 0.633978\n",
      "Loss on epoch 11700: 0.633547\n",
      "Loss on epoch 11800: 0.633118\n",
      "Loss on epoch 11900: 0.632691\n",
      "Loss on epoch 12000: 0.632268\n",
      "Loss on epoch 12100: 0.631847\n",
      "Loss on epoch 12200: 0.631428\n",
      "Loss on epoch 12300: 0.631012\n",
      "Loss on epoch 12400: 0.630598\n",
      "Loss on epoch 12500: 0.630187\n",
      "Loss on epoch 12600: 0.629779\n",
      "Loss on epoch 12700: 0.629372\n",
      "Loss on epoch 12800: 0.628969\n",
      "Loss on epoch 12900: 0.628567\n",
      "Loss on epoch 13000: 0.628169\n",
      "Loss on epoch 13100: 0.627772\n",
      "Loss on epoch 13200: 0.627378\n",
      "Loss on epoch 13300: 0.626986\n",
      "Loss on epoch 13400: 0.626597\n",
      "Loss on epoch 13500: 0.626210\n",
      "Loss on epoch 13600: 0.625826\n",
      "Loss on epoch 13700: 0.625443\n",
      "Loss on epoch 13800: 0.625063\n",
      "Loss on epoch 13900: 0.624686\n",
      "Loss on epoch 14000: 0.624310\n",
      "Loss on epoch 14100: 0.623937\n",
      "Loss on epoch 14200: 0.623566\n",
      "Loss on epoch 14300: 0.623198\n",
      "Loss on epoch 14400: 0.622831\n",
      "Loss on epoch 14500: 0.622467\n",
      "Loss on epoch 14600: 0.622105\n",
      "Loss on epoch 14700: 0.621745\n",
      "Loss on epoch 14800: 0.621388\n",
      "Loss on epoch 14900: 0.621032\n",
      "Loss on epoch 15000: 0.620679\n",
      "Loss on epoch 15100: 0.620328\n",
      "Loss on epoch 15200: 0.619979\n",
      "Loss on epoch 15300: 0.619632\n",
      "Loss on epoch 15400: 0.619287\n",
      "Loss on epoch 15500: 0.618945\n",
      "Loss on epoch 15600: 0.618604\n",
      "Loss on epoch 15700: 0.618266\n",
      "Loss on epoch 15800: 0.617929\n",
      "Loss on epoch 15900: 0.617595\n",
      "Loss on epoch 16000: 0.617262\n",
      "Loss on epoch 16100: 0.616932\n",
      "Loss on epoch 16200: 0.616604\n",
      "Loss on epoch 16300: 0.616277\n",
      "Loss on epoch 16400: 0.615953\n",
      "Loss on epoch 16500: 0.615631\n",
      "Loss on epoch 16600: 0.615310\n",
      "Loss on epoch 16700: 0.614992\n",
      "Loss on epoch 16800: 0.614675\n",
      "Loss on epoch 16900: 0.614361\n",
      "Loss on epoch 17000: 0.614048\n",
      "Loss on epoch 17100: 0.613737\n",
      "Loss on epoch 17200: 0.613428\n",
      "Loss on epoch 17300: 0.613121\n",
      "Loss on epoch 17400: 0.612816\n",
      "Loss on epoch 17500: 0.612513\n",
      "Loss on epoch 17600: 0.612211\n",
      "Loss on epoch 17700: 0.611912\n",
      "Loss on epoch 17800: 0.611614\n",
      "Loss on epoch 17900: 0.611318\n",
      "Loss on epoch 18000: 0.611024\n",
      "Loss on epoch 18100: 0.610732\n",
      "Loss on epoch 18200: 0.610441\n",
      "Loss on epoch 18300: 0.610152\n",
      "Loss on epoch 18400: 0.609865\n",
      "Loss on epoch 18500: 0.609580\n",
      "Loss on epoch 18600: 0.609296\n",
      "Loss on epoch 18700: 0.609015\n",
      "Loss on epoch 18800: 0.608735\n",
      "Loss on epoch 18900: 0.608456\n",
      "Loss on epoch 19000: 0.608180\n",
      "Loss on epoch 19100: 0.607905\n",
      "Loss on epoch 19200: 0.607631\n",
      "Loss on epoch 19300: 0.607360\n",
      "Loss on epoch 19400: 0.607090\n",
      "Loss on epoch 19500: 0.606821\n",
      "Loss on epoch 19600: 0.606555\n",
      "Loss on epoch 19700: 0.606290\n",
      "Loss on epoch 19800: 0.606026\n",
      "Loss on epoch 19900: 0.605764\n",
      "Loss on epoch 20000: 0.605504\n",
      "Loss on epoch 20100: 0.605245\n",
      "Loss on epoch 20200: 0.604988\n",
      "Loss on epoch 20300: 0.604733\n",
      "Loss on epoch 20400: 0.604479\n",
      "Loss on epoch 20500: 0.604227\n",
      "Loss on epoch 20600: 0.603976\n",
      "Loss on epoch 20700: 0.603726\n",
      "Loss on epoch 20800: 0.603478\n",
      "Loss on epoch 20900: 0.603232\n",
      "Loss on epoch 21000: 0.602987\n",
      "Loss on epoch 21100: 0.602744\n",
      "Loss on epoch 21200: 0.602502\n",
      "Loss on epoch 21300: 0.602262\n",
      "Loss on epoch 21400: 0.602023\n",
      "Loss on epoch 21500: 0.601785\n",
      "Loss on epoch 21600: 0.601549\n",
      "Loss on epoch 21700: 0.601315\n",
      "Loss on epoch 21800: 0.601082\n",
      "Loss on epoch 21900: 0.600850\n",
      "Loss on epoch 22000: 0.600620\n",
      "Loss on epoch 22100: 0.600391\n",
      "Loss on epoch 22200: 0.600163\n",
      "Loss on epoch 22300: 0.599937\n",
      "Loss on epoch 22400: 0.599712\n",
      "Loss on epoch 22500: 0.599488\n",
      "Loss on epoch 22600: 0.599266\n",
      "Loss on epoch 22700: 0.599046\n",
      "Loss on epoch 22800: 0.598826\n",
      "Loss on epoch 22900: 0.598608\n",
      "Loss on epoch 23000: 0.598391\n",
      "Loss on epoch 23100: 0.598176\n",
      "Loss on epoch 23200: 0.597962\n",
      "Loss on epoch 23300: 0.597749\n",
      "Loss on epoch 23400: 0.597537\n",
      "Loss on epoch 23500: 0.597327\n",
      "Loss on epoch 23600: 0.597118\n",
      "Loss on epoch 23700: 0.596910\n",
      "Loss on epoch 23800: 0.596703\n",
      "Loss on epoch 23900: 0.596498\n",
      "Loss on epoch 24000: 0.596294\n",
      "Loss on epoch 24100: 0.596091\n",
      "Loss on epoch 24200: 0.595889\n",
      "Loss on epoch 24300: 0.595689\n",
      "Loss on epoch 24400: 0.595489\n",
      "Loss on epoch 24500: 0.595291\n",
      "Loss on epoch 24600: 0.595095\n",
      "Loss on epoch 24700: 0.594899\n",
      "Loss on epoch 24800: 0.594704\n",
      "Loss on epoch 24900: 0.594511\n",
      "Loss on epoch 25000: 0.594319\n",
      "Loss on epoch 25100: 0.594128\n",
      "Loss on epoch 25200: 0.593938\n",
      "Loss on epoch 25300: 0.593749\n",
      "Loss on epoch 25400: 0.593561\n",
      "Loss on epoch 25500: 0.593374\n",
      "Loss on epoch 25600: 0.593189\n",
      "Loss on epoch 25700: 0.593005\n",
      "Loss on epoch 25800: 0.592821\n",
      "Loss on epoch 25900: 0.592639\n",
      "Loss on epoch 26000: 0.592458\n",
      "Loss on epoch 26100: 0.592278\n",
      "Loss on epoch 26200: 0.592099\n",
      "Loss on epoch 26300: 0.591921\n",
      "Loss on epoch 26400: 0.591744\n",
      "Loss on epoch 26500: 0.591568\n",
      "Loss on epoch 26600: 0.591393\n",
      "Loss on epoch 26700: 0.591219\n",
      "Loss on epoch 26800: 0.591046\n",
      "Loss on epoch 26900: 0.590875\n",
      "Loss on epoch 27000: 0.590704\n",
      "Loss on epoch 27100: 0.590534\n",
      "Loss on epoch 27200: 0.590365\n",
      "Loss on epoch 27300: 0.590197\n",
      "Loss on epoch 27400: 0.590031\n",
      "Loss on epoch 27500: 0.589865\n",
      "Loss on epoch 27600: 0.589700\n",
      "Loss on epoch 27700: 0.589536\n",
      "Loss on epoch 27800: 0.589373\n",
      "Loss on epoch 27900: 0.589211\n",
      "Loss on epoch 28000: 0.589050\n",
      "Loss on epoch 28100: 0.588889\n",
      "Loss on epoch 28200: 0.588730\n",
      "Loss on epoch 28300: 0.588572\n",
      "Loss on epoch 28400: 0.588414\n",
      "Loss on epoch 28500: 0.588258\n",
      "Loss on epoch 28600: 0.588102\n",
      "Loss on epoch 28700: 0.587947\n",
      "Loss on epoch 28800: 0.587794\n",
      "Loss on epoch 28900: 0.587641\n",
      "Loss on epoch 29000: 0.587489\n",
      "Loss on epoch 29100: 0.587337\n",
      "Loss on epoch 29200: 0.587187\n",
      "Loss on epoch 29300: 0.587037\n",
      "Loss on epoch 29400: 0.586889\n",
      "Loss on epoch 29500: 0.586741\n",
      "Loss on epoch 29600: 0.586594\n",
      "Loss on epoch 29700: 0.586448\n",
      "Loss on epoch 29800: 0.586302\n",
      "Loss on epoch 29900: 0.586158\n",
      "Loss on epoch 30000: 0.586014\n",
      "Loss on epoch 30100: 0.585871\n",
      "Loss on epoch 30200: 0.585729\n",
      "Loss on epoch 30300: 0.585588\n",
      "Loss on epoch 30400: 0.585447\n",
      "Loss on epoch 30500: 0.585308\n",
      "Loss on epoch 30600: 0.585169\n",
      "Loss on epoch 30700: 0.585031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on epoch 30800: 0.584893\n",
      "Loss on epoch 30900: 0.584757\n",
      "Loss on epoch 31000: 0.584621\n",
      "Loss on epoch 31100: 0.584486\n",
      "Loss on epoch 31200: 0.584351\n",
      "Loss on epoch 31300: 0.584218\n",
      "Loss on epoch 31400: 0.584085\n",
      "Loss on epoch 31500: 0.583953\n",
      "Loss on epoch 31600: 0.583821\n",
      "Loss on epoch 31700: 0.583691\n",
      "Loss on epoch 31800: 0.583561\n",
      "Loss on epoch 31900: 0.583431\n",
      "Loss on epoch 32000: 0.583303\n",
      "Loss on epoch 32100: 0.583175\n",
      "Loss on epoch 32200: 0.583048\n",
      "Loss on epoch 32300: 0.582922\n",
      "Loss on epoch 32400: 0.582796\n",
      "Loss on epoch 32500: 0.582671\n",
      "Loss on epoch 32600: 0.582546\n",
      "Loss on epoch 32700: 0.582423\n",
      "Loss on epoch 32800: 0.582300\n",
      "Loss on epoch 32900: 0.582177\n",
      "Loss on epoch 33000: 0.582055\n",
      "Loss on epoch 33100: 0.581934\n",
      "Loss on epoch 33200: 0.581814\n",
      "Loss on epoch 33300: 0.581694\n",
      "Loss on epoch 33400: 0.581575\n",
      "Loss on epoch 33500: 0.581457\n",
      "Loss on epoch 33600: 0.581339\n",
      "Loss on epoch 33700: 0.581221\n",
      "Loss on epoch 33800: 0.581105\n",
      "Loss on epoch 33900: 0.580989\n",
      "Loss on epoch 34000: 0.580873\n",
      "Loss on epoch 34100: 0.580759\n",
      "Loss on epoch 34200: 0.580645\n",
      "Loss on epoch 34300: 0.580531\n",
      "Loss on epoch 34400: 0.580418\n",
      "Loss on epoch 34500: 0.580306\n",
      "Loss on epoch 34600: 0.580194\n",
      "Loss on epoch 34700: 0.580083\n",
      "Loss on epoch 34800: 0.579972\n",
      "Loss on epoch 34900: 0.579862\n",
      "Loss on epoch 35000: 0.579753\n",
      "Loss on epoch 35100: 0.579644\n",
      "Loss on epoch 35200: 0.579535\n",
      "Loss on epoch 35300: 0.579428\n",
      "Loss on epoch 35400: 0.579321\n",
      "Loss on epoch 35500: 0.579214\n",
      "Loss on epoch 35600: 0.579108\n",
      "Loss on epoch 35700: 0.579002\n",
      "Loss on epoch 35800: 0.578897\n",
      "Loss on epoch 35900: 0.578793\n",
      "Loss on epoch 36000: 0.578689\n",
      "Loss on epoch 36100: 0.578585\n",
      "Loss on epoch 36200: 0.578483\n",
      "Loss on epoch 36300: 0.578380\n",
      "Loss on epoch 36400: 0.578278\n",
      "Loss on epoch 36500: 0.578177\n",
      "Loss on epoch 36600: 0.578076\n",
      "Loss on epoch 36700: 0.577976\n",
      "Loss on epoch 36800: 0.577876\n",
      "Loss on epoch 36900: 0.577777\n",
      "Loss on epoch 37000: 0.577678\n",
      "Loss on epoch 37100: 0.577580\n",
      "Loss on epoch 37200: 0.577482\n",
      "Loss on epoch 37300: 0.577385\n",
      "Loss on epoch 37400: 0.577288\n",
      "Loss on epoch 37500: 0.577192\n",
      "Loss on epoch 37600: 0.577096\n",
      "Loss on epoch 37700: 0.577001\n",
      "Loss on epoch 37800: 0.576906\n",
      "Loss on epoch 37900: 0.576811\n",
      "Loss on epoch 38000: 0.576717\n",
      "Loss on epoch 38100: 0.576624\n",
      "Loss on epoch 38200: 0.576531\n",
      "Loss on epoch 38300: 0.576438\n",
      "Loss on epoch 38400: 0.576346\n",
      "Loss on epoch 38500: 0.576255\n",
      "Loss on epoch 38600: 0.576163\n",
      "Loss on epoch 38700: 0.576073\n",
      "Loss on epoch 38800: 0.575982\n",
      "Loss on epoch 38900: 0.575892\n",
      "Loss on epoch 39000: 0.575803\n",
      "Loss on epoch 39100: 0.575714\n",
      "Loss on epoch 39200: 0.575625\n",
      "Loss on epoch 39300: 0.575537\n",
      "Loss on epoch 39400: 0.575449\n",
      "Loss on epoch 39500: 0.575362\n",
      "Loss on epoch 39600: 0.575275\n",
      "Loss on epoch 39700: 0.575189\n",
      "Loss on epoch 39800: 0.575103\n",
      "Loss on epoch 39900: 0.575017\n",
      "139\n",
      "192\n",
      "0.723958333333\n",
      "hit\n",
      "Loss on epoch 0: 8.870174\n",
      "Loss on epoch 100: 29.033869\n",
      "Loss on epoch 200: 17.006569\n",
      "Loss on epoch 300: nan\n",
      "Loss on epoch 400: nan\n",
      "Loss on epoch 500: nan\n",
      "Loss on epoch 600: nan\n",
      "Loss on epoch 700: nan\n",
      "Loss on epoch 800: nan\n",
      "Loss on epoch 900: nan\n",
      "10436\n",
      "11907\n",
      "0.876459225666\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "num_samples, num_features = m_train.shape        \n",
    "\n",
    "gd_w = np.random.uniform(size=num_features)\n",
    "\n",
    "gd_loss_history = []\n",
    "\n",
    "def gradient_descent_square_error(X,y,epochs=50000,lr=0.001):\n",
    "    num_samples, num_features = X.shape\n",
    "    \n",
    "    for i in xrange(epochs):\n",
    "        global gd_w\n",
    "        predictions = sigmoid(np.dot(X,gd_w))\n",
    "        loss=np.sum((predictions-y)**2)\n",
    "        gd_loss_history.append(loss)\n",
    "    \n",
    "        print(\"[INFO] epoch #{}, loss={:.7f}\".format(i + 1, loss))\n",
    "        \n",
    "        gradient=np.dot(X.T,predictions-y) / num_samples\n",
    "        gd_w+= -gradient * lr\n",
    "\n",
    "def log_loss(X,y):\n",
    "    global gd_w\n",
    "    s = np.dot(X,gd_w)\n",
    "    predictions=sigmoid(s)\n",
    "    \n",
    "    log_l = (-y*np.log(predictions)-(1-y)*np.log(1-predictions)).mean()\n",
    "    return log_l\n",
    "    \n",
    "    \n",
    "def gradient_descent_logistic_reg(X,y,epochs=5000,lr=0.01):\n",
    "    num_samples, num_features = X.shape\n",
    "    \n",
    "    for i in xrange(epochs):\n",
    "        global gd_w\n",
    "        s = np.dot(X,gd_w)\n",
    "        predictions = sigmoid(s)\n",
    "        error = predictions-y\n",
    "        gradient=np.dot(X.T,error)/num_samples\n",
    "        \n",
    "        gd_w += -gradient * lr\n",
    "        if i % 100 == 0:\n",
    "            print \"Loss on epoch %d: %f\"%(i,log_loss(X,y))\n",
    "#             print gd_w\n",
    "    \n",
    "    \n",
    "def check_accuracy(X,y):\n",
    "    num_samples, num_features = X.shape\n",
    "    accurate = 0\n",
    "    global gd_w\n",
    "    \n",
    "    for i in xrange(num_samples):\n",
    "        prediction = sigmoid(np.dot(X[i],gd_w))\n",
    "        label = 0\n",
    "        \n",
    "        if prediction >= .5:\n",
    "            label = 1\n",
    "        \n",
    "       # print label\n",
    "        #print y[i]\n",
    "        if label == y[i]:\n",
    "            accurate += 1\n",
    "    print accurate\n",
    "    print num_samples\n",
    "    return accurate / float(num_samples)\n",
    "\n",
    "# wipe gd_w\n",
    "gradient_descent_logistic_reg(m_train, m_train_labels,40000,0.0001)\n",
    "print (check_accuracy(m_test, m_test_labels))\n",
    "\n",
    "# num_samples, num_features = wbc_train.shape        \n",
    "\n",
    "# gd_w = np.random.uniform(size=num_features)\n",
    "# gradient_descent_logistic_reg(wbc_train, wbc_train_labels)\n",
    "# print (check_accuracy(wbc_test, wbc_test_labels))\n",
    "\n",
    "\n",
    "# num_samples, num_features = news_train.shape        \n",
    "\n",
    "# print \"hit\"\n",
    "# gd_w = np.random.uniform(size=num_features)\n",
    "# gradient_descent_logistic_reg(news_train.toarray(), news_train_labels,1,0.1)\n",
    "# print (check_accuracy(news_test.toarray(), news_test_labels))\n",
    "\n",
    "num_samples, num_features = cod_train.shape        \n",
    "\n",
    "print \"hit\"\n",
    "gd_w = np.random.uniform(size=num_features)\n",
    "gradient_descent_logistic_reg(cod_train.toarray(), cod_train_labels,1000,0.001)\n",
    "print (check_accuracy(cod_test.toarray(), cod_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
