{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "\n",
    "We utilize both external libraries, like numpy and scikit-learn, as well as internally written libraries for the sake of modularity and simplicity of code within this notebook. The goal for modularizing the code base is so that running the different algorithms here can be clean and require as few parameters and extraneous code blocks as possible, enabling us to focus on analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# External libraries:\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Internal libraries:\n",
    "import datasets.data as data\n",
    "from descent_algorithms import *\n",
    "from learning_rates import *\n",
    "from models import *\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "We use three different datasets for our analysis of our algorithms, all of which provide a binary classification problem (??? 0 or 1 ???). (DESCRIPTION OF THREE DATASETS HERE AND HOW THEY ARE PREPPED IN THE data.py FILE)\n",
    "\n",
    "Here, we read in the data vectors and labels using the datasets/data utility functions, and then perform a train/test split of 80%/20% of the provided samples. The splitting is done using the train_test_split function from the sklearn.model_selection package, which randomizes the splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = data.load_wisconsin_breast_cancer()\n",
    "wbc_X_train, wbc_X_test, wbc_y_train, wbc_y_test = train_test_split(\n",
    "    features, labels, test_size=0.2)\n",
    "wbc_n = wbc_X_train.shape[0]\n",
    "\n",
    "M_features, M_labels = data.load_MNIST_13()\n",
    "mnist_X_train, mnist_X_test, mnist_y_train, mnist_y_test = train_test_split(\n",
    "    M_features, M_labels, test_size = 0.2)\n",
    "mnist_n = mnist_X_train.shape[0]\n",
    "\n",
    "cod_features, cod_labels = data.load_cod_rna()\n",
    "cod_X_train, cod_X_test, cod_y_train, cod_y_test = train_test_split(\n",
    "    cod_features, cod_labels, test_size = 0.2)\n",
    "cod_n = cod_X_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparative Measures\n",
    "We use a relative convergence measure of 0.000001 (1/10000% change in loss between iterations), in order to determine whether or not an algorithm has converged. This allows us to directly compare the various descent methods and learning rates (?? and regularizations ??) for convergence rate.\n",
    "\n",
    "Additionally, we keep track of the final loss converged too, the resultant test accuracy, and the time per iteration in order to fully compare the relative performance of the all of the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relative convergence limit\n",
    "rel_conv = 0.000001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed Step Size\n",
    "We begin our analysis with a look at the fixed learning rate convergence for our GD, SGD, AGD, and SVRG algorithms on our three datasets.\n",
    "\n",
    "The default learning rate for fixed is set to 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our learning rate object\n",
    "lr = FixedRate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wisconsin Breast Cancer Data\n",
    "Then, we setup the run for all of our descent methods on the Wisconsin Breast Cancer dataset, beginning with the initialization of each of our descent method objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our descent methods\n",
    "gd = GradientDescent()\n",
    "sgd_1 = GradientDescent() # the GD algorithm is used for all SGD algorithms, \n",
    "                          # with the smaller batch size specified in the model\n",
    "sgd_10 = GradientDescent()\n",
    "sgd_100 = GradientDescent()\n",
    "agd = NesterovAcceleratedDescent()\n",
    "svrg = StochasticVarianceReducedGradientDescent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialize all of our model objects (all logistic regression models in this case), with the appropriate parameters for each algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression(DescentAlgorithm, LearningRate, max iterations, \n",
    "# batch size, relative convergence)\n",
    "gd_log = LogisticRegression(gd, lr, 5000, wbc_n, rel_conv)\n",
    "sgd_1_log = LogisticRegression(sgd_1, lr, 2000, 1, rel_conv)\n",
    "sgd_10_log = LogisticRegression(sgd_10, lr, 4000, 10, rel_conv)\n",
    "sgd_100_log = LogisticRegression(sgd_100, lr, 4000, 100, rel_conv)\n",
    "agd_log = LogisticRegression(agd, lr, 400, wbc_n, rel_conv)\n",
    "svrg_log = LogisticRegression(svrg, lr, 20, wbc_n, rel_conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we run the fit for each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting gradient descent:\n",
      "Iter:        0 train loss: 375.565\n",
      "Iter:      500 train loss: 234.202\n",
      "Iter:     1000 train loss: 227.359\n",
      "Iter:     1500 train loss: 225.602\n",
      "Iter:     2000 train loss: 224.971\n",
      "Iter:     2500 train loss: 224.705\n",
      "Converged in 2788 iterations.\n",
      "\n",
      "Fitting stochastic gradient descent, batch size = 1:\n",
      "Iter:        0 train loss: 392.547\n",
      "Iter:      200 train loss: 256.957\n",
      "Iter:      400 train loss: 238.522\n",
      "Iter:      600 train loss: 232.521\n",
      "Iter:      800 train loss: 325.748\n",
      "Converged in 987 iterations.\n",
      "\n",
      "Fitting stochastic gradient descent, batch size = 10:\n",
      "Iter:        0 train loss: 372.393\n",
      "Iter:      400 train loss: 238.371\n",
      "Iter:      800 train loss: 228.928\n",
      "Iter:     1200 train loss: 226.759\n",
      "Converged in 1461 iterations.\n",
      "\n",
      "Fitting stochastic gradient descent, batch size = 100:\n",
      "Iter:        0 train loss: 382.632\n",
      "Iter:      400 train loss: 237.869\n",
      "Converged in 480 iterations.\n",
      "\n",
      "Fitting accelerated gradient descent:\n",
      "Iter:        0 train loss: 386.776\n",
      "Iter:       40 train loss: 243.251\n",
      "Iter:       80 train loss: 226.067\n",
      "Iter:      120 train loss: 224.846\n",
      "Converged in 158 iterations.\n",
      "\n",
      "Fitting stochastic variance reduced gradient descent:\n",
      "Iter:        0 train loss: 258.395\n",
      "Iter:        2 train loss: 224.572\n",
      "Iter:        4 train loss: 224.472\n",
      "Iter:        6 train loss: 224.469\n",
      "Iter:        8 train loss: 224.469\n",
      "Converged in 8 iterations.\n"
     ]
    }
   ],
   "source": [
    "print('Fitting gradient descent:')\n",
    "wbc_gd_loss = gd_log.fit(wbc_X_train, wbc_y_train)\n",
    "print('\\nFitting stochastic gradient descent, batch size = 1:')\n",
    "wbc_sgd_1_loss = sgd_1_log.fit(wbc_X_train, wbc_y_train)\n",
    "print('\\nFitting stochastic gradient descent, batch size = 10:')\n",
    "wbc_sgd_10_loss = sgd_10_log.fit(wbc_X_train, wbc_y_train)\n",
    "print('\\nFitting stochastic gradient descent, batch size = 100:')\n",
    "wbc_sgd_100_loss = sgd_100_log.fit(wbc_X_train, wbc_y_train)\n",
    "print('\\nFitting accelerated gradient descent:')\n",
    "wbc_agd_loss = agd_log.fit(wbc_X_train, wbc_y_train)\n",
    "print('\\nFitting stochastic variance reduced gradient descent:')\n",
    "wbc_svrg_loss = svrg_log.fit(wbc_X_train, wbc_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD Accuracy: 86.43%\n",
      "SGD 1 Accuracy: 77.14%\n",
      "SGD 10 Accuracy: 86.43%\n",
      "SGD 100 Accuracy: 86.43%\n",
      "AGD Accuracy: 87.86%\n",
      "SVRG Accuracy: 87.86%\n"
     ]
    }
   ],
   "source": [
    "acc = check_accuracy(gd_log, wbc_X_test, wbc_y_test)\n",
    "print(\"GD Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy(sgd_1_log, wbc_X_test, wbc_y_test)\n",
    "print(\"SGD 1 Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy(sgd_10_log, wbc_X_test, wbc_y_test)\n",
    "print(\"SGD 10 Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy(sgd_100_log, wbc_X_test, wbc_y_test)\n",
    "print(\"SGD 100 Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy(agd_log, wbc_X_test, wbc_y_test)\n",
    "print(\"AGD Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy(svrg_log, wbc_X_test, wbc_y_test)\n",
    "print(\"SVRG Accuracy: {0:.2f}%\".format(acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'figsize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-94a6af62fc89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwbc_gd_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwbc_sgd_1_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwbc_sgd_10_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwbc_sgd_100_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwbc_agd_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwbc_svrg_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwbc_agd_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/c/Users/klink/Documents/Machine Learning Optimization/project/mlo_final_project/util.py\u001b[0m in \u001b[0;36mplot_losses\u001b[0;34m(gd, sgd_1, sgd_10, sgd_100, agd, svrg, md)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd_10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd_100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Iteration Number'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'xx-large'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'figsize' is not defined"
     ]
    }
   ],
   "source": [
    "plot_losses(wbc_gd_loss, wbc_sgd_1_loss, wbc_sgd_10_loss, wbc_sgd_100_loss, wbc_agd_loss, wbc_svrg_loss, wbc_agd_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MNIST Data\n",
    "Then, we setup the run for all of our descent methods on the MNIST dataset, beginning with the initialization of each of our descent method objects. We combine the cells here and reduce the footprint, as the usage is the same as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = FixedRate(0.000001)\n",
    "# initialize our descent methods\n",
    "gd = GradientDescent()\n",
    "sgd_1 = GradientDescent() \n",
    "sgd_10 = GradientDescent()\n",
    "sgd_100 = GradientDescent()\n",
    "agd = NesterovAcceleratedDescent()\n",
    "svrd = StochasticVarianceReducedGradientDescent()\n",
    "# initialize the logisitic regression objects\n",
    "gd_log = LogisticRegression(gd, lr, 2000, mnist_n, rel_conv)\n",
    "sgd_1_log = LogisticRegression(sgd_1, lr, 100, 1, rel_conv)\n",
    "sgd_10_log = LogisticRegression(sgd_10, lr, 200, 10, rel_conv)\n",
    "sgd_100_log = LogisticRegression(sgd_100, lr, 2000, 100, rel_conv)\n",
    "agd_log = LogisticRegression(agd, lr, 200, mnist_n, rel_conv)\n",
    "svrd_log = LogisticRegression(svrd, lr, 20, mnist_n, rel_conv)\n",
    "# and run the fit for each of these models, this time on the MNIST data set:\n",
    "print('Fitting gradient descent:')\n",
    "mnist_gd_loss = gd_log.fit(mnist_X_train, mnist_y_train, non_zero_init = True)\n",
    "print('\\nFitting stochastic gradient descent, batch size = 1:')\n",
    "mnist_sgd_1_loss = sgd_1_log.fit(mnist_X_train, mnist_y_train, non_zero_init = True)\n",
    "print('\\nFitting stochastic gradient descent, batch size = 10:')\n",
    "mnist_sgd_10_loss = sgd_10_log.fit(mnist_X_train, mnist_y_train, non_zero_init = True)\n",
    "print('\\nFitting stochastic gradient descent, batch size = 100:')\n",
    "mnist_sgd_100_loss = sgd_100_log.fit(mnist_X_train, mnist_y_train, non_zero_init = True)\n",
    "print('\\nFitting accelerated gradient descent:')\n",
    "mnist_agd_loss = agd_log.fit(mnist_X_train, mnist_y_train, non_zero_init = True)\n",
    "print('\\nFitting stochastic variance reduced gradient descent:')\n",
    "mnist_sbrd_loss = svrd_log.fit(mnist_X_train, mnist_y_train, non_zero_init = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COD-RNA Data\n",
    "Lastly, we setup the run for all of our descent methods on the COD-RNA dataset, again using a reduced-frill cell to run our fit for each method's model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = FixedRate(0.001)\n",
    "# initialize our descent methods\n",
    "gd = GradientDescent()\n",
    "sgd_1 = GradientDescent()\n",
    "sgd_10 = GradientDescent()\n",
    "sgd_100 = GradientDescent()\n",
    "agd = NesterovAcceleratedDescent()\n",
    "svrd = StochasticVarianceReducedGradientDescent()\n",
    "# initialize the logisitic regression objects\n",
    "gd_log = LogisticRegression(gd, lr, 5000, wbc_n, rel_conv)\n",
    "sgd_1_log = LogisticRegression(sgd_1, lr, 2000, 1, rel_conv)\n",
    "sgd_10_log = LogisticRegression(sgd_10, lr, 2000, 10, rel_conv)\n",
    "sgd_100_log = LogisticRegression(sgd_100, lr, 2000, 100, rel_conv)\n",
    "agd_log = LogisticRegression(agd, lr, 200, wbc_n, rel_conv)\n",
    "svrd_log = LogisticRegression(svrd, lr, 20, wbc_n, rel_conv)\n",
    "# and run the fit for each of these models, this time on the MNIST data set:\n",
    "print('Fitting gradient descent:')\n",
    "wbc_gd_loss = gd_log.fit(wbc_X_train, wbc_y_train, non_zero_init = True)\n",
    "print('\\nFitting stochastic gradient descent, batch size = 1:')\n",
    "wbc_sgd_1_loss = sgd_1_log.fit(wbc_X_train, wbc_y_train, non_zero_init = True)\n",
    "print('\\nFitting stochastic gradient descent, batch size = 10:')\n",
    "wbc_sgd_10_loss = sgd_10_log.fit(wbc_X_train, wbc_y_train, non_zero_init = True)\n",
    "print('\\nFitting stochastic gradient descent, batch size = 100:')\n",
    "wbc_sgd_100_loss = sgd_100_log.fit(wbc_X_train, wbc_y_train, non_zero_init = True)\n",
    "print('\\nFitting accelerated gradient descent:')\n",
    "wbc_agd_loss = agd_log.fit(wbc_X_train, wbc_y_train, non_zero_init = True)\n",
    "print('\\nFitting stochastic variance reduced gradient descent:')\n",
    "wbc_sbrd_loss = svrd_log.fit(wbc_X_train, wbc_y_train, non_zero_init = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialize all of our model objects (all logistic regression models in this case), with the appropriate parameters for each algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression(DescentAlgorithm, LearningRate, max iterations, \n",
    "# batch size, relative convergence)\n",
    "gd_log = LogisticRegression(gd, lr, 5000, wbc_n, rel_conv)\n",
    "sgd_1_log = LogisticRegression(sgd_1, lr, 2000, 1, rel_conv)\n",
    "sgd_10_log = LogisticRegression(sgd_10, lr, 2000, 10, rel_conv)\n",
    "sgd_100_log = LogisticRegression(sgd_100, lr, 2000, 100, rel_conv)\n",
    "agd_log = LogisticRegression(agd, lr, 200, wbc_n, rel_conv)\n",
    "svrd_log = LogisticRegression(svrd, lr, 20, wbc_n, rel_conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we run the fit for each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Fitting gradient descent:')\n",
    "wbc_gd_loss = gd_log.fit(wbc_X_train, wbc_y_train, non_zero_init = True)\n",
    "print('\\nFitting stochastic gradient descent, batch size = 1:')\n",
    "wbc_sgd_1_loss = sgd_1_log.fit(wbc_X_train, wbc_y_train, non_zero_init = True)\n",
    "print('\\nFitting stochastic gradient descent, batch size = 10:')\n",
    "wbc_sgd_10_loss = sgd_10_log.fit(wbc_X_train, wbc_y_train, non_zero_init = True)\n",
    "print('\\nFitting stochastic gradient descent, batch size = 100:')\n",
    "wbc_sgd_100_loss = sgd_100_log.fit(wbc_X_train, wbc_y_train, non_zero_init = True)\n",
    "print('\\nFitting accelerated gradient descent:')\n",
    "wbc_agd_loss = agd_log.fit(wbc_X_train, wbc_y_train, non_zero_init = True)\n",
    "print('\\nFitting stochastic variance reduced gradient descent:')\n",
    "wbc_sbrd_loss = svrd_log.fit(wbc_X_train, wbc_y_train, non_zero_init = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = check_accuracy(log, wbc_X_test, wbc_y_test)\n",
    "print(\"Model Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "\n",
    "plt.figure(1, figsize=(12, 6))\n",
    "plt.title('Loss Plot')\n",
    "plt.xlabel('Iteration Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(loss, 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run notes:\n",
    "\n",
    "FIRST:\n",
    "lr:\n",
    "    - fixed: GD(0.01), SGD(0.01) - batched at 1,10,100 , SVRG(0.01), Nest(0.01)\n",
    "    - polydecay: GD(0.01, 0.0001), SGD(0.01, 0.00001) - batched, SVRG(N/A), Nest(N/A)\n",
    "    - expdecay: GD(0.1,0.001), SGD(0.1,.001) - batched, SVRG(N/A), Nest(N/A)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
