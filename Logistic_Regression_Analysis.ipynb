{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Group Members:\n",
    "Peter Klinkmueller, Archan Patel, Will Ye, Jason Zhang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "For our project, we seek to compare the convergence rates of a number of descent algorithms for two different objectives, over multiple data sets, and utilizing three different learning rates.\n",
    "\n",
    "`Will`\n",
    "\n",
    "{ there was no where really else to put this TODO, so: pls comment the rest of the modules as JZ has comments so far for the descent_methods.py file - NOTE: you __don't__ have to do any commenting to util.py or for testing_mean.py - just the descent methods file, the learning rates file, and the models file - use the same formatting as JZ's existing comments, just for consistency, ty bb }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "In this section of our report, the different objectives, descent algorithms, and learning rates are discussed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Objectives:\n",
    "We implemented two different objectives for our project - logsitic regression and a support vector machine.\n",
    "\n",
    "   __Logistic Regression__\n",
    "\n",
    "   `Will`\n",
    "\n",
    "   { insert latex for the loss function for logistic regression here (the formulation that we use in the code) }\n",
    "\n",
    "   { insert latex for the gradient function for logistic regression here (again, the one from the code) }\n",
    "\n",
    "   `Archan`\n",
    "\n",
    "   { insert a brief discussion of logistic regression here...why we used it, what makes it nice, idk anything else you wanna say }\n",
    " \n",
    "   __Support Vector Machine__\n",
    "\n",
    "   `Will`\n",
    "\n",
    "   { insert latex for the loss function for SVM here (the formulation that we use in the code) }\n",
    "\n",
    "   { insert latex for the gradient function for SVM here (again, the one from the code) }\n",
    "\n",
    "   `Archan`\n",
    "\n",
    "   { insert a brief discussion of SVM here, same deal as for with logistic regression }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Descent Methods\n",
    "We implemented four different descent methods (with the objects taking in a batch size, which allowed for easy modification of gradient descent to be used to implement stochastic gradient descent).\n",
    "\n",
    "   __Gradient Descent__\n",
    "   This is our baseline descent method, with the implementation using the usual simple update rule.\n",
    "\n",
    "   __Stochastic Gradient Descent__\n",
    "   As noted above, we simply used the GradientDescent descent method object to run a model using stochastic gradient descent by    simply changing the batch size to be smaller than the number of training samples. We run mini-batches as well, testing       stochastic gradient descent for batch sizes: $\\{1, 10, 100\\}$.\n",
    "\n",
    "   __Nesterov's Accelerated Gradient Descent__\n",
    "\n",
    "   `Archan/JZ - whoever actually wrote this/knows it - you guys figure it out`\n",
    "\n",
    "   { insert a brief discussion of how this descent method works, and what we expect to get out of it compared to GD }\n",
    "\n",
    "   __Stochastic Variance Reduced Gradient Descent__\n",
    "  \n",
    "   `Peter`\n",
    "\n",
    "   { insert a brief discussion of how this descent method works, and what we expect to get out of it compared to GD }\n",
    "\n",
    "  __Mirror Descent__\n",
    "\n",
    "   `JZ`\n",
    "\n",
    "   { insert a brief discussion of how this descent method works, and what we expect to get out of it compared to GD }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Learning Rates\n",
    "We explored four different learning rates.\n",
    "   __Fixed__ \n",
    "   The fixed learning rate simply sets and uses a constant $\\eta$ for the step size throughout the model fit.\n",
    "   __Square Root Decay__\n",
    "   The square root decay we implemented is of the form:\n",
    "   $$ \\eta_i = \\eta * \\frac{1}{\\sqrt{i}} $$\n",
    "   where $\\eta$ is a set parameter and $i$ is the iteration number.\n",
    "   __Exponential Decay__\n",
    "   The exponential decay we implemented is of the form:\n",
    "   $$ \\eta_i = \\eta * \\frac{1}{\\gamma * i} ,$$\n",
    "   where $\\eta$ and $\\gamma$ are set parameters and $i$ is the iteration number.\n",
    "\n",
    "   We wanted to see if this has a better or worse accelerating effect on the convergence of the algorithms compared to the square root decay scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Relative Convergence Condition\n",
    "In order to perform our analysis, we relay upon a relative convergence check performed on the loss, $L$ after each iteration of a descent algorithm. This check is of the form:\n",
    "$$ \\frac{| L_{i-1} - L_i |}{L_i} < rel\\_conv $$\n",
    "where $rel\\_conv$ is our relative convergence condition, which we set as $rel\\_conv = 0.000001$, and $L_i$ is our loss at iteration $i$.\n",
    "\n",
    "   This allows us to run each model fit up until it reaches this convergence, thus enabling a direct comparison on convergence rate by analyzing the relative runtime of each algorithm. This also means that all of our algorithms will converge to very close to the same loss, and thus the same accuracy, for each objective function/data set pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "Here, we discuss how we went about implementing our methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Overview\n",
    "`Will`\n",
    "\n",
    "{ insert some general discussion of our goals for implementing our methods... modularization...plug-n-play (know you love that)...etc. }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Objectives\n",
    "`Will`\n",
    "\n",
    "{ insert brief discussion of modularization of objective objects }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Descent Methods\n",
    "`Will`\n",
    "\n",
    "{ insert brief discussion of modularization of descent method objects }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Learning Rates\n",
    "`Will`\n",
    "\n",
    "{ insert brief discussion of modularization of learning rate objects }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "We utilize both external libraries, like numpy and scikit-learn, as well as internally written libraries for the sake of modularity and simplicity of code within this notebook. The goal for modularizing the code base is so that running the different algorithms here can be clean and require as few parameters and extraneous code blocks as possible, enabling us to focus on analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External libraries:\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Internal libraries:\n",
    "import datasets.data as data\n",
    "from descent_algorithms import *\n",
    "from learning_rates import *\n",
    "from models import *\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "We use three different datasets for our analysis of our algorithms, all of which provide a binary classification problem.\n",
    "\n",
    "`Will`\n",
    "\n",
    "{ insert discussion of the data sets - focusing on why they meet the needs of our problem and what process you had to go through to prep them to be usable for this purpose }\n",
    "\n",
    "Here, we read in the data vectors and labels using the datasets/data utility functions, and then perform a train/test split of 80%/20% of the provided samples. The splitting is done using the train_test_split function from the sklearn.model_selection package, which randomizes the splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = data.load_wisconsin_breast_cancer()\n",
    "wbc_X_train, wbc_X_test, wbc_y_train, wbc_y_test = train_test_split(\n",
    "    features, labels, test_size=0.2)\n",
    "wbc_n = wbc_X_train.shape[0]\n",
    "\n",
    "M_features, M_labels = data.load_MNIST_13()\n",
    "mnist_X_train, mnist_X_test, mnist_y_train, mnist_y_test = train_test_split(\n",
    "    M_features, M_labels, test_size = 0.2)\n",
    "mnist_n = mnist_X_train.shape[0]\n",
    "\n",
    "cod_features, cod_labels = data.load_cod_rna()\n",
    "cod_X_train, cod_X_test, cod_y_train, cod_y_test = train_test_split(\n",
    "    cod_features, cod_labels, test_size = 0.2)\n",
    "cod_n = cod_X_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Relative Convergence Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relative convergence limit\n",
    "rel_conv = 0.000001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wisconsin Breast Cancer Data Set\n",
    "We begin our analysis with a look at the performance of our three $LearningRate$ types on the WBC data set for all of our descent algorithms, which we will abbreviate for the remainder of the analysis as: Gradient Descent (GD), Stochastic Gradient Descent (SGD), Nesterov's Accelerated Gradient Descent (AGD), Stochastic Variance Reduced Gradient Descent (SVRG), and Mirror Descent (MD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixed Learning Rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our learning rate object\n",
    "lr = FixedRate(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by instantiating our descent method objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate our descent methods\n",
    "gd = GradientDescent()\n",
    "sgd_1 = GradientDescent() # the GD algorithm is used for all SGD algorithms, \n",
    "                          # with the smaller batch size specified in the model\n",
    "sgd_10 = GradientDescent()\n",
    "sgd_100 = GradientDescent()\n",
    "agd = NesterovAcceleratedDescent()\n",
    "svrg = StochasticVarianceReducedGradientDescent()\n",
    "md = MirrorDescent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialize all of our model objects (all logistic regression models in this case), with the appropriate parameters for each algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression(DescentAlgorithm, LearningRate, max iterations, batch size, relative convergence)\n",
    "gd_log = LogisticRegression(gd, lr, 5000, wbc_n, rel_conv)\n",
    "sgd_1_log = LogisticRegression(sgd_1, lr, 2000, 1, rel_conv)\n",
    "sgd_10_log = LogisticRegression(sgd_10, lr, 4000, 10, rel_conv)\n",
    "sgd_100_log = LogisticRegression(sgd_100, lr, 4000, 100, rel_conv)\n",
    "agd_log = LogisticRegression(agd, lr, 400, wbc_n, rel_conv)\n",
    "svrg_log = LogisticRegression(svrg, lr, 20, wbc_n, rel_conv)\n",
    "md_log = LogisticRegression(md, lr, 2000, wbc_n, rel_conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we run the fit for each model (10 runs each, providing better results in the mean across the runs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting gradient descent:\n",
      "Iter:        0 train loss: 375.002\n",
      "Iter:      500 train loss: 225.254\n",
      "Iter:     1000 train loss: 216.193\n",
      "Iter:     1500 train loss: 213.715\n",
      "Iter:     2000 train loss: 212.824\n",
      "Iter:     2500 train loss: 212.458\n",
      "Iter:     3000 train loss: 212.296\n",
      "Converged in 3008 iterations.\n",
      "\n",
      "Fitting stochastic gradient descent, batch size = 1:\n",
      "Iter:        0 train loss: 407.684\n",
      "Iter:      200 train loss: 274.196\n",
      "Iter:      400 train loss: 249.848\n",
      "Iter:      600 train loss: 222.718\n",
      "Iter:      800 train loss: 220.777\n",
      "Iter:     1000 train loss: 221.910\n",
      "Iter:     1200 train loss: 277.182\n",
      "Converged in 1266 iterations.\n",
      "\n",
      "Fitting stochastic gradient descent, batch size = 10:\n",
      "Iter:        0 train loss: 379.870\n",
      "Iter:      400 train loss: 229.315\n",
      "Iter:      800 train loss: 218.308\n",
      "Iter:     1200 train loss: 215.550\n",
      "Iter:     1600 train loss: 213.982\n",
      "Iter:     2000 train loss: 212.983\n",
      "Iter:     2400 train loss: 213.703\n",
      "Iter:     2800 train loss: 212.829\n",
      "Iter:     3200 train loss: 212.374\n",
      "Iter:     3600 train loss: 212.347\n",
      "\n",
      "Fitting stochastic gradient descent, batch size = 100:\n",
      "Iter:        0 train loss: 372.859\n",
      "Converged in 237 iterations.\n",
      "\n",
      "Fitting accelerated gradient descent:\n",
      "Iter:        0 train loss: 1371.701\n",
      "Iter:       40 train loss: nan\n",
      "Iter:       80 train loss: nan\n",
      "Iter:      120 train loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/klink/Documents/Machine Learning Optimization/project/mlo_final_project/models.py:75: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.dot(-y.T, np.log(h)) - np.dot((1 - y).T,np.log(1 - h))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:      160 train loss: nan\n",
      "Iter:      200 train loss: nan\n",
      "Iter:      240 train loss: nan\n",
      "Iter:      280 train loss: nan\n",
      "Iter:      320 train loss: nan\n",
      "Iter:      360 train loss: nan\n",
      "\n",
      "Fitting stochastic variance reduced gradient descent:\n",
      "Iter:        0 train loss: 240.360\n",
      "Iter:        2 train loss: 217.426\n",
      "Iter:        4 train loss: 212.961\n",
      "Iter:        6 train loss: 212.175\n",
      "Iter:        8 train loss: 212.150\n",
      "Iter:       10 train loss: 212.149\n",
      "Converged in 10 iterations.\n",
      "\n",
      "Fitting mirror descent:\n",
      "Converged in 38 iterations.\n"
     ]
    }
   ],
   "source": [
    "print('Fitting gradient descent:')\n",
    "gd_loss, gd_time = gd_log.fit(wbc_X_train, wbc_y_train, True)\n",
    "print('\\nFitting stochastic gradient descent, batch size = 1:')\n",
    "sgd_1_loss, sgd_1_time = sgd_1_log.fit(wbc_X_train, wbc_y_train, True)\n",
    "print('\\nFitting stochastic gradient descent, batch size = 10:')\n",
    "sgd_10_loss, sgd_10_time = sgd_10_log.fit(wbc_X_train, wbc_y_train, True)\n",
    "print('\\nFitting stochastic gradient descent, batch size = 100:')\n",
    "sgd_100_loss, sgd_100_time = sgd_100_log.fit(wbc_X_train, wbc_y_train, True)\n",
    "print('\\nFitting accelerated gradient descent:')\n",
    "agd_loss, agd_time = agd_log.fit(wbc_X_train, wbc_y_train, True)\n",
    "print('\\nFitting stochastic variance reduced gradient descent:')\n",
    "svrg_loss, svrg_time = svrg_log.fit(wbc_X_train, wbc_y_train, True)\n",
    "# for i in range(0,wbc_svrg_loss.shape[0]*wbc_n):\n",
    "#     if \n",
    "print('\\nFitting mirror descent:')\n",
    "md_loss, md_time = md_log.fit(wbc_X_train, wbc_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD Accuracy: 84.29%\n",
      "SGD 1 Accuracy: 87.14%\n",
      "SGD 10 Accuracy: 84.29%\n",
      "SGD 100 Accuracy: 85.71%\n",
      "AGD Accuracy: 85.00%\n",
      "SVRG Accuracy: 84.29%\n",
      "MD Accuracy: 84.29%\n"
     ]
    }
   ],
   "source": [
    "acc = check_accuracy(gd_log, wbc_X_test, wbc_y_test)\n",
    "print(\"GD Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy(sgd_1_log, wbc_X_test, wbc_y_test)\n",
    "print(\"SGD 1 Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy(sgd_10_log, wbc_X_test, wbc_y_test)\n",
    "print(\"SGD 10 Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy(sgd_100_log, wbc_X_test, wbc_y_test)\n",
    "print(\"SGD 100 Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy(agd_log, wbc_X_test, wbc_y_test)\n",
    "print(\"AGD Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy(svrg_log, wbc_X_test, wbc_y_test)\n",
    "print(\"SVRG Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy(md_log, wbc_X_test, wbc_y_test)\n",
    "print(\"MD Accuracy: {0:.2f}%\".format(acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we re-run this fitting for each of the algorithms nine more times in order to better smooth our expected loss results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 3008 iterations.\n",
      "Converged in 650 iterations.\n",
      "Converged in 2489 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 8 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 896 iterations.\n",
      "Converged in 2617 iterations.\n",
      "Converged in 2087 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 8 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1283 iterations.\n",
      "Converged in 1398 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 10 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 834 iterations.\n",
      "Converged in 2690 iterations.\n",
      "Converged in 467 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 8 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 727 iterations.\n",
      "Converged in 393 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 8 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1054 iterations.\n",
      "Converged in 440 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 8 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 641 iterations.\n",
      "Converged in 826 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 7 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1171 iterations.\n",
      "Converged in 658 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 8 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1305 iterations.\n",
      "Converged in 952 iterations.\n",
      "Converged in 703 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 8 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 465 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 11 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 913 iterations.\n",
      "Converged in 1084 iterations.\n",
      "Converged in 877 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 7 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1423 iterations.\n",
      "Converged in 102 iterations.\n",
      "Converged in 428 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 7 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 909 iterations.\n",
      "Converged in 2332 iterations.\n",
      "Converged in 366 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 9 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1744 iterations.\n",
      "Converged in 507 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 7 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1124 iterations.\n",
      "Converged in 841 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 10 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1009 iterations.\n",
      "Converged in 1015 iterations.\n",
      "Converged in 221 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 9 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1303 iterations.\n",
      "Converged in 1348 iterations.\n",
      "Converged in 1439 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 8 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 537 iterations.\n",
      "Converged in 1492 iterations.\n",
      "Converged in 709 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 9 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 883 iterations.\n",
      "Converged in 219 iterations.\n",
      "Converged in 931 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 8 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 858 iterations.\n",
      "Converged in 813 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 10 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1678 iterations.\n",
      "Converged in 906 iterations.\n",
      "Converged in 1755 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 10 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1476 iterations.\n",
      "Converged in 1639 iterations.\n",
      "Converged in 307 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 6 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1557 iterations.\n",
      "Converged in 1913 iterations.\n",
      "Converged in 799 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 9 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1445 iterations.\n",
      "Converged in 754 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 8 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1407 iterations.\n",
      "Converged in 1099 iterations.\n",
      "Converged in 420 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 7 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 883 iterations.\n",
      "Converged in 493 iterations.\n",
      "Converged in 509 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 7 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1007 iterations.\n",
      "Converged in 2992 iterations.\n",
      "Converged in 488 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 11 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1237 iterations.\n",
      "Converged in 1376 iterations.\n",
      "Converged in 767 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 8 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1092 iterations.\n",
      "Converged in 1043 iterations.\n",
      "Converged in 460 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 6 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1533 iterations.\n",
      "Converged in 1906 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 7 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1378 iterations.\n",
      "Converged in 3095 iterations.\n",
      "Converged in 738 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 11 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 762 iterations.\n",
      "Converged in 1264 iterations.\n",
      "Converged in 281 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 7 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 785 iterations.\n",
      "Converged in 3558 iterations.\n",
      "Converged in 620 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 7 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1595 iterations.\n",
      "Converged in 1864 iterations.\n",
      "Converged in 800 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 10 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1091 iterations.\n",
      "Converged in 1183 iterations.\n",
      "Converged in 605 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 10 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1345 iterations.\n",
      "Converged in 1054 iterations.\n",
      "Converged in 799 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 5 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 761 iterations.\n",
      "Converged in 1156 iterations.\n",
      "Converged in 413 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 8 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1585 iterations.\n",
      "Converged in 3270 iterations.\n",
      "Converged in 358 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 7 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1308 iterations.\n",
      "Converged in 251 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 7 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1072 iterations.\n",
      "Converged in 1610 iterations.\n",
      "Converged in 849 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 8 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1604 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 11 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1383 iterations.\n",
      "Converged in 1625 iterations.\n",
      "Converged in 653 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 9 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1106 iterations.\n",
      "Converged in 476 iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 861 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 7 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 890 iterations.\n",
      "Converged in 659 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 5 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1080 iterations.\n",
      "Converged in 1562 iterations.\n",
      "Converged in 202 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 9 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 784 iterations.\n",
      "Converged in 835 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 7 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1320 iterations.\n",
      "Converged in 547 iterations.\n",
      "Converged in 675 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 7 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 814 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 8 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 883 iterations.\n",
      "Converged in 542 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 7 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1328 iterations.\n",
      "Converged in 1086 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 8 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 687 iterations.\n",
      "Converged in 485 iterations.\n",
      "Converged in 604 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 8 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1632 iterations.\n",
      "Converged in 1324 iterations.\n",
      "Converged in 492 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 8 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 126 iterations.\n",
      "Converged in 1379 iterations.\n",
      "Converged in 369 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 10 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 889 iterations.\n",
      "Converged in 1270 iterations.\n",
      "Converged in 3288 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 8 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1968 iterations.\n",
      "Converged in 3515 iterations.\n",
      "Converged in 564 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 10 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1243 iterations.\n",
      "Converged in 566 iterations.\n",
      "Converged in 155 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 8 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 728 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 13 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1099 iterations.\n",
      "Converged in 1860 iterations.\n",
      "Converged in 166 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 7 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1076 iterations.\n",
      "Converged in 1985 iterations.\n",
      "Converged in 573 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 8 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1112 iterations.\n",
      "Converged in 1766 iterations.\n",
      "Converged in 932 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 8 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1307 iterations.\n",
      "Converged in 397 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 10 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 945 iterations.\n",
      "Converged in 2051 iterations.\n",
      "Converged in 377 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 8 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1082 iterations.\n",
      "Converged in 800 iterations.\n",
      "Converged in 775 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 10 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 949 iterations.\n",
      "Converged in 536 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 7 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 960 iterations.\n",
      "Converged in 1637 iterations.\n",
      "Converged in 712 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 14 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 727 iterations.\n",
      "Converged in 704 iterations.\n",
      "Converged in 719 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 9 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1162 iterations.\n",
      "Converged in 1328 iterations.\n",
      "Converged in 486 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 8 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1924 iterations.\n",
      "Converged in 1092 iterations.\n",
      "Converged in 497 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 7 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1426 iterations.\n",
      "Converged in 647 iterations.\n",
      "Converged in 360 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 11 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1471 iterations.\n",
      "Converged in 1707 iterations.\n",
      "Converged in 389 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 9 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1033 iterations.\n",
      "Converged in 473 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 9 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1107 iterations.\n",
      "Converged in 1794 iterations.\n",
      "Converged in 350 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 9 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1087 iterations.\n",
      "Converged in 854 iterations.\n",
      "Converged in 523 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 8 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1220 iterations.\n",
      "Converged in 1429 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 8 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 814 iterations.\n",
      "Converged in 2590 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 9 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1220 iterations.\n",
      "Converged in 578 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 8 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 816 iterations.\n",
      "Converged in 1191 iterations.\n",
      "Converged in 462 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 7 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 863 iterations.\n",
      "Converged in 3471 iterations.\n",
      "Converged in 929 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 9 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1226 iterations.\n",
      "Converged in 3272 iterations.\n",
      "Converged in 1436 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 8 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 818 iterations.\n",
      "Converged in 1349 iterations.\n",
      "Converged in 617 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 10 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 849 iterations.\n",
      "Converged in 1569 iterations.\n",
      "Converged in 755 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 8 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1003 iterations.\n",
      "Converged in 503 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 8 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1202 iterations.\n",
      "Converged in 478 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 8 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1465 iterations.\n",
      "Converged in 579 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 11 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 996 iterations.\n",
      "Converged in 868 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 9 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 1237 iterations.\n",
      "Converged in 2427 iterations.\n",
      "Converged in 592 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 8 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 736 iterations.\n",
      "Converged in 963 iterations.\n",
      "Converged in 2579 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 8 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 674 iterations.\n",
      "Converged in 573 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 10 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 734 iterations.\n",
      "Converged in 2744 iterations.\n",
      "Converged in 954 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 7 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1409 iterations.\n",
      "Converged in 523 iterations.\n",
      "Converged in 215 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 8 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1681 iterations.\n",
      "Converged in 932 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 9 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 666 iterations.\n",
      "Converged in 419 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 10 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1464 iterations.\n",
      "Converged in 937 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 7 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1077 iterations.\n",
      "Converged in 1108 iterations.\n",
      "Converged in 1010 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 7 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1171 iterations.\n",
      "Converged in 1871 iterations.\n",
      "Converged in 1107 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 8 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1262 iterations.\n",
      "Converged in 402 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 7 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 977 iterations.\n",
      "Converged in 644 iterations.\n",
      "Converged in 1000 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 9 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1481 iterations.\n",
      "Converged in 89 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 9 iterations.\n",
      "Converged in 1665 iterations.\n",
      "Converged in 3008 iterations.\n",
      "Converged in 1445 iterations.\n",
      "Converged in 670 iterations.\n",
      "Converged in 131 iterations.\n",
      "Converged in 6 iterations.\n",
      "Converged in 1665 iterations.\n"
     ]
    }
   ],
   "source": [
    "gd_loss_counts = np.sign(gd_loss)\n",
    "sgd_1_loss_counts = np.sign(sgd_1_loss)\n",
    "sgd_10_loss_counts = np.sign(sgd_10_loss)\n",
    "sgd_100_loss_counts = np.sign(sgd_100_loss)\n",
    "agd_loss_counts = np.sign(agd_loss)\n",
    "svrg_loss_counts = np.sign(svrg_loss)\n",
    "md_loss_counts = np.sign(md_loss)\n",
    "\n",
    "for i in range(0,99):\n",
    "    gd = GradientDescent()\n",
    "    sgd_1 = GradientDescent()\n",
    "    sgd_10 = GradientDescent()\n",
    "    sgd_100 = GradientDescent()\n",
    "    agd = NesterovAcceleratedDescent()\n",
    "    svrg = StochasticVarianceReducedGradientDescent()\n",
    "    md = MirrorDescent()\n",
    "    gd_log = LogisticRegression(gd, lr, 5000, wbc_n, rel_conv)\n",
    "    sgd_1_log = LogisticRegression(sgd_1, lr, 2000, 1, rel_conv)\n",
    "    sgd_10_log = LogisticRegression(sgd_10, lr, 4000, 10, rel_conv)\n",
    "    sgd_100_log = LogisticRegression(sgd_100, lr, 4000, 100, rel_conv)\n",
    "    agd_log = LogisticRegression(agd, lr, 400, wbc_n, rel_conv)\n",
    "    svrg_log = LogisticRegression(svrg, lr, 20, wbc_n, rel_conv)\n",
    "    md_log = LogisticRegression(md, lr, 2000, wbc_n, rel_conv)\n",
    "    \n",
    "    tmp_loss, tmp_time = gd_log.fit(wbc_X_train, wbc_y_train)\n",
    "    gd_loss_counts += np.sign(tmp_loss)\n",
    "    gd_loss += tmp_loss\n",
    "    gd_time += tmp_time\n",
    "    tmp_loss, tmp_time = sgd_1_log.fit(wbc_X_train, wbc_y_train)\n",
    "    sgd_1_loss_counts += np.sign(tmp_loss)\n",
    "    sgd_1_loss += tmp_loss\n",
    "    sgd_1_time += tmp_time\n",
    "    tmp_loss, tmp_time = sgd_10_log.fit(wbc_X_train, wbc_y_train)\n",
    "    sgd_10_loss_counts += np.sign(tmp_loss)\n",
    "    sgd_10_loss += tmp_loss\n",
    "    sgd_10_time += tmp_time\n",
    "    tmp_loss, tmp_time = sgd_100_log.fit(wbc_X_train, wbc_y_train)\n",
    "    sgd_100_loss_counts += np.sign(tmp_loss)\n",
    "    sgd_100_loss += tmp_loss\n",
    "    sgd_100_time += tmp_time\n",
    "    tmp_loss, tmp_time = agd_log.fit(wbc_X_train, wbc_y_train)\n",
    "    agd_loss_counts += np.sign(tmp_loss)\n",
    "    agd_loss += tmp_loss\n",
    "    agd_time += tmp_time\n",
    "    tmp_loss, tmp_time = svrg_log.fit(wbc_X_train, wbc_y_train)\n",
    "    svrg_loss_counts += np.sign(tmp_loss)\n",
    "    svrg_loss += tmp_loss\n",
    "    svrg_time += tmp_time\n",
    "    tmp_loss, tmp_time = md_log.fit(wbc_X_train, wbc_y_train)\n",
    "    md_loss_counts += np.sign(tmp_loss)\n",
    "    md_loss += tmp_loss\n",
    "    md_time += tmp_time\n",
    "gd_loss /= gd_loss_counts\n",
    "sgd_1_loss /= sgd_1_loss_counts\n",
    "sgd_10_loss /= sgd_10_loss_counts\n",
    "sgd_100_loss /= sgd_100_loss_counts\n",
    "agd_loss /= agd_loss_counts\n",
    "svrg_loss /= svrg_loss_counts\n",
    "md_loss /= md_loss_counts\n",
    "gd_time /= 100\n",
    "sgd_1_time /= 100\n",
    "sgd_10_time /= 100\n",
    "sgd_100_time /= 100\n",
    "agd_time /= 100\n",
    "svrg_time /= 100\n",
    "md_time /= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.89495224237442\n",
      "0.5146340107917786\n",
      "0.5572686982154846\n",
      "0.30148998737335203\n",
      "0.12963653802871705\n",
      "0.8682884550094605\n",
      "1.2997781491279603\n"
     ]
    }
   ],
   "source": [
    "gd_loss = gd_loss[gd_loss.nonzero()]\n",
    "sgd_1_loss = sgd_1_loss[sgd_1_loss.nonzero()]\n",
    "sgd_10_loss = sgd_10_loss[sgd_10_loss.nonzero()]\n",
    "sgd_100_loss = sgd_100_loss[sgd_100_loss.nonzero()]\n",
    "agd_loss = agd_loss[agd_loss.nonzero()]\n",
    "svrg_loss = svrg_loss[svrg_loss.nonzero()]\n",
    "md_loss = md_loss[md_loss.nonzero()]\n",
    "print(gd_time)\n",
    "print(sgd_1_time)\n",
    "print(sgd_10_time)\n",
    "print(sgd_100_time)\n",
    "print(agd_time)\n",
    "print(svrg_time)\n",
    "print(md_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can plot the run-averaged losses for each algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_fixed_losses(gd_loss, sgd_1_loss, sgd_10_loss, sgd_100_loss, agd_loss, svrg_loss, md_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our learning rate object\n",
    "lr_gd = ExpDecayRate(0.1, 0.0001)\n",
    "lr_sgd = ExpDecayRate(0.01, 0.00001)\n",
    "lr_md = ExpDecayRate(0.1, 0.00001)\n",
    "# initialize our descent methods\n",
    "gd = GradientDescent()\n",
    "sgd_1 = GradientDescent()\n",
    "md = MirrorDescent()\n",
    "# initialize logistic regression models\n",
    "gd_log = LogisticRegression(gd, lr_gd, 2000, wbc_n, rel_conv)\n",
    "sgd_1_log = LogisticRegression(sgd_1, lr_sgd, 2000, 1, rel_conv)\n",
    "md_log = LogisticRegression(md, lr_md, 2000, wbc_n, rel_conv)\n",
    "# fit the models...\n",
    "print('Fitting gradient descent:')\n",
    "wbc_gd_loss = gd_log.fit(wbc_X_train, wbc_y_train)\n",
    "print('\\nFitting stochastic gradient descent, batch size = 1:')\n",
    "wbc_sgd_1_loss = sgd_1_log.fit(wbc_X_train, wbc_y_train)\n",
    "print('\\nFitting mirror descent:')\n",
    "wbc_md_loss = md_log.fit(wbc_X_train, wbc_y_train)\n",
    "# print the test accuracies for each model\n",
    "acc = check_accuracy(gd_log, wbc_X_test, wbc_y_test)\n",
    "print(\"\\n\\nGD Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy(sgd_1_log, wbc_X_test, wbc_y_test)\n",
    "print(\"SGD 1 Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy(md_log, wbc_X_test, wbc_y_test)\n",
    "print(\"MD Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "# plot the loss convergences for each model\n",
    "plot_dynamic_losses(wbc_gd_loss, wbc_sgd_1_loss, wbc_md_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our learning rate object\n",
    "# lr_gd = ExpDecayRate(0.1, 0.001)\n",
    "# lr_sgd = ExpDecayRate(0.01, 0.001)\n",
    "# lr_md = ExpDecayRate(0.1, 0.001)\n",
    "lr_gd = SqrtDecayRate(0.001,1)\n",
    "lr_sgd = SqrtDecayRate(0.0001,1)\n",
    "lr_md = SqrtDecayRate(0.001,1)\n",
    "# initialize our descent methods\n",
    "gd = GradientDescent()\n",
    "sgd_1 = GradientDescent()\n",
    "md = MirrorDescent()\n",
    "# initialize logistic regression models\n",
    "gd_log = LogisticRegression(gd, lr_gd, 2000, wbc_n, rel_conv)\n",
    "sgd_1_log = LogisticRegression(sgd_1, lr_sgd, 2000, 1, rel_conv)\n",
    "md_log = LogisticRegression(md, lr_md, 2000, wbc_n, rel_conv)\n",
    "# fit the models...\n",
    "print('Fitting gradient descent:')\n",
    "wbc_gd_loss = gd_log.fit(wbc_X_train, wbc_y_train)\n",
    "print('\\nFitting stochastic gradient descent, batch size = 1:')\n",
    "wbc_sgd_1_loss = sgd_1_log.fit(wbc_X_train, wbc_y_train)\n",
    "print('\\nFitting mirror descent:')\n",
    "wbc_md_loss = md_log.fit(wbc_X_train, wbc_y_train)\n",
    "# print the test accuracies for each model\n",
    "acc = check_accuracy(gd_log, wbc_X_test, wbc_y_test)\n",
    "print(\"\\n\\nGD Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy(sgd_1_log, wbc_X_test, wbc_y_test)\n",
    "print(\"SGD 1 Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy(md_log, wbc_X_test, wbc_y_test)\n",
    "print(\"MD Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "# plot the loss convergences for each model\n",
    "plot_dynamic_losses(wbc_gd_loss, wbc_sgd_1_loss, wbc_md_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MNIST Data\n",
    "Then, we setup the run for all of our descent methods on the MNIST dataset, beginning with the initialization of each of our descent method objects. We combine the cells here and reduce the footprint, as the usage is the same as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr = FixedRate(0.000001)\n",
    "# initialize our descent methods\n",
    "gd = GradientDescent()\n",
    "sgd_1 = GradientDescent() \n",
    "sgd_10 = GradientDescent()\n",
    "sgd_100 = GradientDescent()\n",
    "agd = NesterovAcceleratedDescent()\n",
    "svrg = StochasticVarianceReducedGradientDescent()\n",
    "md = MirrorDescent()\n",
    "# initialize the logisitic regression objects\n",
    "gd_log = LogisticRegression(gd, lr, 5000, mnist_n, rel_conv)\n",
    "sgd_1_log = LogisticRegression(sgd_1, lr, 5000, 1, rel_conv)\n",
    "sgd_10_log = LogisticRegression(sgd_10, lr, 5000, 10, rel_conv)\n",
    "sgd_100_log = LogisticRegression(sgd_100, lr, 5000, 100, rel_conv)\n",
    "agd_log = LogisticRegression(agd, lr, 2000, mnist_n, rel_conv)\n",
    "svrg_log = LogisticRegression(svrg, lr, 40, mnist_n, rel_conv)\n",
    "md_log = LogisticRegression(md, lr, 3000, mnist_n, rel_conv)\n",
    "# and run the fit for each of these models, this time on the MNIST data set:\n",
    "print('Fitting gradient descent:')\n",
    "mnist_gd_loss = gd_log.fit(mnist_X_train, mnist_y_train)\n",
    "print('\\nFitting stochastic gradient descent, batch size = 1:')\n",
    "mnist_sgd_1_loss = sgd_1_log.fit(mnist_X_train, mnist_y_train)\n",
    "print('\\nFitting stochastic gradient descent, batch size = 10:')\n",
    "mnist_sgd_10_loss = sgd_10_log.fit(mnist_X_train, mnist_y_train)\n",
    "print('\\nFitting stochastic gradient descent, batch size = 100:')\n",
    "mnist_sgd_100_loss = sgd_100_log.fit(mnist_X_train, mnist_y_train)\n",
    "print('\\nFitting accelerated gradient descent:')\n",
    "mnist_agd_loss = agd_log.fit(mnist_X_train, mnist_y_train)\n",
    "print('\\nFitting stochastic variance reduced gradient descent:')\n",
    "mnist_svrg_loss = svrg_log.fit(mnist_X_train, mnist_y_train)\n",
    "print('\\nFitting mirror descent:')\n",
    "mnist_md_loss = md_log.fit(mnist_X_train, mnist_y_train)\n",
    "# displaying accuracies\n",
    "acc = check_accuracy(gd_log, mnist_X_test, mnist_y_test)\n",
    "print(\"\\n\\nGD Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy(sgd_1_log, mnist_X_test, mnist_y_test)\n",
    "print(\"SGD 1 Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy(sgd_10_log, mnist_X_test, mnist_y_test)\n",
    "print(\"SGD 10 Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy(sgd_100_log, mnist_X_test, mnist_y_test)\n",
    "print(\"SGD 100 Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy(agd_log, mnist_X_test, mnist_y_test)\n",
    "print(\"AGD Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy(svrg_log, mnist_X_test, mnist_y_test)\n",
    "print(\"SVRG Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy(md_log, mnist_X_test, mnist_y_test)\n",
    "print(\"MD Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "# plot losses\n",
    "plot_losses(mnist_gd_loss, mnist_sgd_1_loss, mnist_sgd_10_loss, \n",
    "            mnist_sgd_100_loss, mnist_agd_loss, mnist_svrg_loss, \n",
    "            mnist_md_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our learning rate object\n",
    "lr_gd = PolyDecayRate(0.01, 0.0001)\n",
    "lr_sgd = PolyDecayRate(0.01, 0.00001)\n",
    "lr_md = PolyDecayRate(0.1, 0.00001)\n",
    "# initialize our descent methods\n",
    "gd = GradientDescent()\n",
    "sgd_1 = GradientDescent()\n",
    "md = MirrorDescent()\n",
    "# initialize logistic regression models\n",
    "gd_log = LogisticRegression(gd, lr_gd, 2000, mnist_n, rel_conv)\n",
    "sgd_1_log = LogisticRegression(sgd_1, lr_sgd, 2000, 1, rel_conv)\n",
    "md_log = LogisticRegression(md, lr_md, 2000, mnist_n, rel_conv)\n",
    "# fit the models...\n",
    "print('Fitting gradient descent:')\n",
    "mnist_gd_loss = gd_log.fit(mnist_X_train, mnist_y_train)\n",
    "print('\\nFitting stochastic gradient descent, batch size = 1:')\n",
    "mnist_sgd_1_loss = sgd_1_log.fit(mnist_X_train, mnist_y_train)\n",
    "print('\\nFitting mirror descent:')\n",
    "mnist_md_loss = md_log.fit(mnist_X_train, mnist_y_train)\n",
    "# print the test accuracies for each model\n",
    "acc = check_accuracy(gd_log, mnist_X_test, mnist_y_test)\n",
    "print(\"\\n\\nGD Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy(sgd_1_log, mnist_X_test, mnist_y_test)\n",
    "print(\"SGD 1 Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy(md_log, mnist_X_test, mnist_y_test)\n",
    "print(\"MD Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "# plot the loss convergences for each model\n",
    "plot_dynamic_losses(mnist_gd_loss, mnist_sgd_1_loss, mnist_md_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COD-RNA Data\n",
    "Lastly, we setup the run for all of our descent methods on the COD-RNA dataset, again using a reduced-frill cell to run our fit for each method's model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = FixedRate(0.00001)\n",
    "# initialize our descent methods\n",
    "gd = GradientDescent()\n",
    "sgd_1 = GradientDescent()\n",
    "sgd_10 = GradientDescent()\n",
    "sgd_100 = GradientDescent()\n",
    "agd = NesterovAcceleratedDescent()\n",
    "svrg = StochasticVarianceReducedGradientDescent()\n",
    "md = MirrorDescent()\n",
    "# initialize the logisitic regression objects\n",
    "gd_log = LogisticRegression(gd, lr, 5000, cod_n, rel_conv)\n",
    "sgd_1_log = LogisticRegression(sgd_1, lr, 4000, 1, rel_conv)\n",
    "sgd_10_log = LogisticRegression(sgd_10, lr, 4000, 10, rel_conv)\n",
    "sgd_100_log = LogisticRegression(sgd_100, lr, 4000, 100, rel_conv)\n",
    "agd_log = LogisticRegression(agd, lr, 200, cod_n, rel_conv)\n",
    "svrg_log = LogisticRegression(svrg, lr, 20, cod_n, rel_conv)\n",
    "md_log = LogisticRegression(md, lr, 2000, cod_n, rel_conv)\n",
    "# and run the fit for each of these models, this time on the MNIST data set:\n",
    "print('Fitting gradient descent:')\n",
    "cod_gd_loss = gd_log.fit(cod_X_train, cod_y_train)\n",
    "print('\\nFitting stochastic gradient descent, batch size = 1:')\n",
    "cod_sgd_1_loss = sgd_1_log.fit(cod_X_train, cod_y_train)\n",
    "print('\\nFitting stochastic gradient descent, batch size = 10:')\n",
    "cod_sgd_10_loss = sgd_10_log.fit(cod_X_train, cod_y_train)\n",
    "print('\\nFitting stochastic gradient descent, batch size = 100:')\n",
    "cod_sgd_100_loss = sgd_100_log.fit(cod_X_train, cod_y_train)\n",
    "print('\\nFitting accelerated gradient descent:')\n",
    "cod_agd_loss = agd_log.fit(cod_X_train, cod_y_train)\n",
    "print('\\nFitting stochastic variance reduced gradient descent:')\n",
    "cod_svrg_loss = svrg_log.fit(cod_X_train, cod_y_train)    \n",
    "print('\\nFitting stochastic variance reduced gradient descent:')\n",
    "cod_md_loss = md_log.fit(cod_X_train, cod_y_train)\n",
    "# displaying accuracies\n",
    "acc = check_accuracy(gd_log, cod_X_test, cod_y_test)\n",
    "print(\"\\n\\nGD Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy(sgd_1_log, cod_X_test, cod_y_test)\n",
    "print(\"SGD 1 Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy(sgd_10_log, cod_X_test, cod_y_test)\n",
    "print(\"SGD 10 Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy(sgd_100_log, cod_X_test, cod_y_test)\n",
    "print(\"SGD 100 Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy(agd_log, cod_X_test, cod_y_test)\n",
    "print(\"AGD Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy(svrg_log, cod_X_test, cod_y_test)\n",
    "print(\"SVRG Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy(md_log, cod_X_test, cod_y_test)\n",
    "print(\"MD Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "# plot losses\n",
    "plot_fixed_losses(cod_gd_loss, cod_sgd_1_loss, cod_sgd_10_loss, \n",
    "            cod_sgd_100_loss, cod_agd_loss, cod_svrg_loss, \n",
    "            cod_md_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our learning rate object\n",
    "lr_gd = PolyDecayRate(0.0001, 0.0001)\n",
    "lr_sgd = PolyDecayRate(0.00001, 0.00001)\n",
    "lr_md = PolyDecayRate(0.0001, 0.00001)\n",
    "# initialize our descent methods\n",
    "gd = GradientDescent()\n",
    "sgd_1 = GradientDescent()\n",
    "md = MirrorDescent()\n",
    "# initialize logistic regression models\n",
    "gd_log = LogisticRegression(gd, lr_gd, 1000, cod_n, rel_conv)\n",
    "sgd_1_log = LogisticRegression(sgd_1, lr_sgd, 1500, 1, rel_conv)\n",
    "md_log = LogisticRegression(md, lr_md, 1000, cod_n, rel_conv)\n",
    "# fit the models...\n",
    "print('Fitting gradient descent:')\n",
    "cod_gd_loss = gd_log.fit(cod_X_train, cod_y_train)\n",
    "print('\\nFitting stochastic gradient descent, batch size = 1:')\n",
    "cod_sgd_1_loss = sgd_1_log.fit(cod_X_train, cod_y_train)\n",
    "print('\\nFitting mirror descent:')\n",
    "cod_md_loss = md_log.fit(cod_X_train, cod_y_train)\n",
    "# print the test accuracies for each model\n",
    "acc = check_accuracy(gd_log, cod_X_test, cod_y_test)\n",
    "print(\"\\n\\nGD Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy(sgd_1_log, cod_X_test, cod_y_test)\n",
    "print(\"SGD 1 Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy(md_log, cod_X_test, cod_y_test)\n",
    "print(\"MD Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "# plot the loss convergences for each model\n",
    "plot_dynamic_losses(cod_gd_loss, cod_sgd_1_loss, cod_md_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our learning rate object\n",
    "lr_gd = SqrtDecayRate(0.0001, 1.)\n",
    "lr_sgd = SqrtDecayRate(0.00001, 1.)\n",
    "lr_md = SqrtDecayRate(0.0001, 1.)\n",
    "# initialize our descent methods\n",
    "gd = GradientDescent()\n",
    "sgd_1 = GradientDescent()\n",
    "md = MirrorDescent()\n",
    "# initialize logistic regression models\n",
    "gd_log = LogisticRegression(gd, lr_gd, 2000, cod_n, rel_conv)\n",
    "sgd_1_log = LogisticRegression(sgd_1, lr_sgd, 4000, 1, rel_conv)\n",
    "md_log = LogisticRegression(md, lr_md, 4000, cod_n, rel_conv)\n",
    "# fit the models...\n",
    "print('Fitting gradient descent:')\n",
    "wbc_gd_loss = gd_log.fit(cod_X_train, cod_y_train)\n",
    "print('\\nFitting stochastic gradient descent, batch size = 1:')\n",
    "wbc_sgd_1_loss = sgd_1_log.fit(cod_X_train, cod_y_train)\n",
    "print('\\nFitting mirror descent:')\n",
    "wbc_md_loss = md_log.fit(cod_X_train, cod_y_train)\n",
    "# print the test accuracies for each model\n",
    "acc = check_accuracy(gd_log, cod_X_test, cod_y_test)\n",
    "print(\"\\n\\nGD Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy(sgd_1_log, cod_X_test, cod_y_test)\n",
    "print(\"SGD 1 Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy(md_log, cod_X_test, cod_y_test)\n",
    "print(\"MD Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "# plot the loss convergences for each model\n",
    "plot_dynamic_losses(wbc_gd_loss, wbc_sgd_1_loss, wbc_md_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential Decaying Learning Rate\n",
    "We begin our analysis with a look at the polynomial decaying learning rate convergence for our GD, SGD, AGD, and SVRG algorithms on our three datasets.\n",
    "\n",
    "The default learning rate for fixed is set to 0.01, and a gamma value of 0.0001."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wisconsin Breast Cancer Data\n",
    "Then, we setup the run for all of our descent methods on the Wisconsin Breast Cancer dataset, beginning with the initialization of each of our descent method objects."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### MNIST Data\n",
    "Then, we setup the run for all of our descent methods on the MNIST dataset, beginning with the initialization of each of our descent method objects. We combine the cells here and reduce the footprint, as the usage is the same as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COD-RNA Data\n",
    "Lastly, we setup the run for all of our descent methods on the COD-RNA dataset, again using a reduced-frill cell to run our fit for each method's model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Square-Root Decaying Learning Rate\n",
    "Lastly, we analyze an exponentially decaying learning rate for convergence for our GD, SGD, AGD, and SVRG algorithms on our three datasets.\n",
    "\n",
    "The default learning rate for fixed is set to 0.01, and a gamma value of 0.0001."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wisconsin Breast Cancer Data\n",
    "Then, we setup the run for all of our descent methods on the Wisconsin Breast Cancer dataset, beginning with the initialization of each of our descent method objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run notes:\n",
    "\n",
    "FIRST:\n",
    "lr:\n",
    "    - fixed: GD(0.01), SGD(0.01) - batched at 1,10,100 , SVRG(0.01), Nest(0.01)\n",
    "    - polydecay: GD(0.01, 0.0001), SGD(0.01, 0.00001) - batched, SVRG(N/A), Nest(N/A)\n",
    "    - expdecay: GD(0.1,0.001), SGD(0.1,.001) - batched, SVRG(N/A), Nest(N/A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will carry out our analysis of the SVM objective.\n",
    "\n",
    "We will perform largely the same analysis as with out logistic regression test, however, we will use a mini-batch SGD algorithm for the dynamic, decaying learning rates instead of the single SGD algorithm.\n",
    "\n",
    "Again, a relative convergence of $rel\\_conv = 0.000001$ will be used in order to compare the relative convergence rates of the different descent methods.\n",
    "\n",
    "The hyperparameter $c$ was found to value $0.00001$. This parameter is ...\n",
    "## ARCHAN \n",
    "{ pls insert a brief explanation for what the heck this 'c' is / what is does }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0.00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can begin running our analysis, however, we need to convert our data sets' labels to be $[-1,1]$, rather than how they are currently configured as $[0,1]$. This is accomplished by running the train and test split label vectors through a simply utility function we have written for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label conversion for the Wisconsin Breast Cancer data set\n",
    "wbc_y_train = zero_one_labels_to_signed(wbc_y_train)\n",
    "wbc_y_test = zero_one_labels_to_signed(wbc_y_test)\n",
    "\n",
    "# label conversion for the MNIST binarized data set\n",
    "mnist_y_train = zero_one_labels_to_signed(mnist_y_train)\n",
    "mnist_y_test = zero_one_labels_to_signed(mnist_y_test)\n",
    "\n",
    "# lavel conversion for the COD-RNA data set\n",
    "cod_y_train = zero_one_labels_to_signed(cod_y_train)\n",
    "cod_y_test = zero_one_labels_to_signed(cod_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with Logistic Regression, we begin with observing the performance of the SVM objective across the different algorithms for the different learning rate paradigms investigated, and then across the three data sets. The configuration of the executed cells is the same as with the Logistic Regression, but with SVM model objects instantiated in place of the LogisticRegression ones.\n",
    "\n",
    "### Wisconsin Breast Cancer Data Set\n",
    "We begin our SVM analysis by observing the algorithms' performance on the WBC data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fixed Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate our learning rate object\n",
    "lr = FixedRate(0.001)\n",
    "\n",
    "# instantiate our descent methods\n",
    "gd = GradientDescent()\n",
    "sgd_100 = GradientDescent()\n",
    "agd = NesterovAcceleratedDescent()\n",
    "svrg = StochasticVarianceReducedGradientDescent()\n",
    "md = MirrorDescent()\n",
    "\n",
    "# instantiate all of the SVM model objects\n",
    "gd_svm = SVM(gd, lr, c, 20000, wbc_n, rel_conv)\n",
    "sgd_100_svm = SVM(sgd_100, lr, c, 20000, 100, rel_conv)\n",
    "agd_svm = SVM(agd, lr, c, 20000, wbc_n, rel_conv)\n",
    "svrg_svm = SVM(svrg, lr, c, 3000, wbc_n, rel_conv)\n",
    "md_svm = SVM(md, lr, c, 2000, wbc_n, rel_conv)\n",
    "\n",
    "# run fitting for all of the models\n",
    "print('Fitting gradient descent:')\n",
    "wbc_gd_loss = gd_svm.fit(wbc_X_train, wbc_y_train)\n",
    "print('\\nFitting stochastic gradient descent, batch size = 100:')\n",
    "wbc_sgd_100_loss = sgd_100_svm.fit(wbc_X_train, wbc_y_train)\n",
    "print('\\nFitting accelerated gradient descent:')\n",
    "wbc_agd_loss = agd_svm.fit(wbc_X_train, wbc_y_train)\n",
    "print('\\nFitting stochastic variance reduced gradient descent:')\n",
    "wbc_svrg_loss = svrg_svm.fit(wbc_X_train, wbc_y_train)\n",
    "print('\\nFitting mirror descent:')\n",
    "wbc_md_loss = md_svm.fit(wbc_X_train, wbc_y_train)\n",
    "\n",
    "# print test accuracies\n",
    "acc = check_accuracy_svm(gd_svm, wbc_X_test, wbc_y_test)\n",
    "print(\"GD Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy_svm(sgd_100_svm, wbc_X_test, wbc_y_test)\n",
    "print(\"SGD 100 Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy_svm(agd_svm, wbc_X_test, wbc_y_test)\n",
    "print(\"AGD Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy_svm(svrg_svm, wbc_X_test, wbc_y_test)\n",
    "print(\"SVRG Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy_svm(md_svm, wbc_X_test, wbc_y_test)\n",
    "print(\"MD Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "\n",
    "# plot our losses\n",
    "plot_fixed_svm_losses(wbc_gd_loss, wbc_sgd_100_loss, wbc_agd_loss, wbc_svrg_loss, wbc_agd_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exponential Decay Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our learning rate object\n",
    "lr_gd = PolyDecayRate(0.01, 0.0001)\n",
    "lr_sgd = PolyDecayRate(0.01, 0.00001)\n",
    "lr_md = PolyDecayRate(0.001, 0.00001)\n",
    "# initialize our descent methods\n",
    "gd = GradientDescent()\n",
    "sgd_100 = GradientDescent()\n",
    "md = MirrorDescent()\n",
    "# initialize logistic regression models\n",
    "gd_log = SVM(gd, lr_gd, c, 1000, wbc_n, rel_conv)\n",
    "sgd_100_log = SVM(sgd_100, lr_sgd, c, 2000, 100, rel_conv)\n",
    "md_log = SVM(md, lr_md, c, 1000, wbc_n, rel_conv)\n",
    "# fit the models...\n",
    "print('Fitting gradient descent:')\n",
    "wbc_gd_loss = gd_log.fit(wbc_X_train, wbc_y_train)\n",
    "print('\\nFitting stochastic gradient descent, batch size = 1:')\n",
    "wbc_sgd_100_loss = sgd_100_log.fit(wbc_X_train, wbc_y_train)\n",
    "print('\\nFitting mirror descent:')\n",
    "wbc_md_loss = md_log.fit(wbc_X_train, wbc_y_train)\n",
    "# print the test accuracies for each model\n",
    "acc = check_accuracy_svm(gd_log, wbc_X_test, wbc_y_test)\n",
    "print(\"\\n\\nGD Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy_svm(sgd_100_log, wbc_X_test, wbc_y_test)\n",
    "print(\"SGD 100 Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy_svm(md_log, wbc_X_test, wbc_y_test)\n",
    "print(\"MD Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "# plot the loss convergences for each model\n",
    "plot_dynamic_svm_losses(wbc_gd_loss, wbc_sgd_100_loss, wbc_md_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Square Root Decay Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our learning rate object\n",
    "lr_gd = ExpDecayRate(0.01, 0.001)\n",
    "lr_sgd = ExpDecayRate(0.01, 0.0001)\n",
    "lr_md = ExpDecayRate(0.001, 0.001)\n",
    "\n",
    "# initialize our descent methods\n",
    "gd = GradientDescent()\n",
    "sgd_100 = GradientDescent()\n",
    "md = MirrorDescent()\n",
    "\n",
    "# initialize logistic regression models\n",
    "gd_log = SVM(gd, lr_gd, c, 2000, wbc_n, rel_conv)\n",
    "sgd_100_log = SVM(sgd_100, lr_sgd, c, 6000, 100, rel_conv)\n",
    "md_log = SVM(md, lr_md, c, 2000, wbc_n, rel_conv)\n",
    "\n",
    "# fit the models...\n",
    "print('Fitting gradient descent:')\n",
    "wbc_gd_loss = gd_log.fit(wbc_X_train, wbc_y_train)\n",
    "print('\\nFitting stochastic gradient descent, batch size = 1:')\n",
    "wbc_sgd_100_loss = sgd_100_log.fit(wbc_X_train, wbc_y_train)\n",
    "print('\\nFitting mirror descent:')\n",
    "wbc_md_loss = md_log.fit(wbc_X_train, wbc_y_train)\n",
    "\n",
    "# print the test accuracies for each model\n",
    "acc = check_accuracy_svm(gd_log, wbc_X_test, wbc_y_test)\n",
    "print(\"\\n\\nGD Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy_svm(sgd_100_log, wbc_X_test, wbc_y_test)\n",
    "print(\"SGD 1 Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy_svm(md_log, wbc_X_test, wbc_y_test)\n",
    "print(\"MD Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "\n",
    "# plot the loss convergences for each model\n",
    "plot_dynamic_svm_losses(wbc_gd_loss, wbc_sgd_100_loss, wbc_md_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see for the WBC data that our algorithms...\n",
    "# Peter \n",
    "{ insert analysis for previous three plots and results here...\n",
    "  \n",
    "  also insert a chart for the timing and iterations to convergence }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarized MNIST Data Set\n",
    "We now investigate the same algorithms and learning rates performance on the MNIST data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fixed Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our learning rate object\n",
    "lr = FixedRate(0.001)\n",
    "\n",
    "# initialize our descent method objects\n",
    "gd = GradientDescent()\n",
    "sgd_100 = GradientDescent()\n",
    "agd = NesterovAcceleratedDescent()\n",
    "svrg = StochasticVarianceReducedGradientDescent()\n",
    "md = MirrorDescent()\n",
    "\n",
    "# initialize all of the SVM models\n",
    "gd_svm = SVM(gd, lr, c, 20000, wbc_n, rel_conv)\n",
    "sgd_100_svm = SVM(sgd_100, lr, c, 20000, 100, rel_conv)\n",
    "agd_svm = SVM(agd, lr, c, 20000, wbc_n, rel_conv)\n",
    "svrg_svm = SVM(svrg, lr, c, 3000, wbc_n, rel_conv)\n",
    "md_svm = SVM(md, lr, c, 2000, wbc_n, rel_conv)\n",
    "\n",
    "# run fitting for all of the models\n",
    "print('Fitting gradient descent:')\n",
    "wbc_gd_loss = gd_svm.fit(wbc_X_train, wbc_y_train)\n",
    "print('\\nFitting stochastic gradient descent, batch size = 100:')\n",
    "wbc_sgd_100_loss = sgd_100_svm.fit(wbc_X_train, wbc_y_train)\n",
    "print('\\nFitting accelerated gradient descent:')\n",
    "wbc_agd_loss = agd_svm.fit(wbc_X_train, wbc_y_train)\n",
    "print('\\nFitting stochastic variance reduced gradient descent:')\n",
    "wbc_svrg_loss = svrg_svm.fit(wbc_X_train, wbc_y_train)\n",
    "print('\\nFitting mirror descent:')\n",
    "wbc_md_loss = md_svm.fit(wbc_X_train, wbc_y_train)\n",
    "\n",
    "# print test accuracies\n",
    "acc = check_accuracy_svm(gd_svm, wbc_X_test, wbc_y_test)\n",
    "print(\"GD Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy_svm(sgd_100_svm, wbc_X_test, wbc_y_test)\n",
    "print(\"SGD 100 Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy_svm(agd_svm, wbc_X_test, wbc_y_test)\n",
    "print(\"AGD Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy_svm(svrg_svm, wbc_X_test, wbc_y_test)\n",
    "print(\"SVRG Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy_svm(md_svm, wbc_X_test, wbc_y_test)\n",
    "print(\"MD Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "\n",
    "plot_fixed_svm_losses(wbc_gd_loss, wbc_sgd_100_loss, wbc_agd_loss, wbc_svrg_loss, wbc_agd_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exponential Decay Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our learning rate object\n",
    "lr_gd = PolyDecayRate(0.01, 0.0001)\n",
    "lr_sgd = PolyDecayRate(0.01, 0.00001)\n",
    "lr_md = PolyDecayRate(0.001, 0.00001)\n",
    "# initialize our descent methods\n",
    "gd = GradientDescent()\n",
    "sgd_100 = GradientDescent()\n",
    "md = MirrorDescent()\n",
    "# initialize logistic regression models\n",
    "gd_log = SVM(gd, lr_gd, c, 1000, wbc_n, rel_conv)\n",
    "sgd_100_log = SVM(sgd_100, lr_sgd, c, 2000, 100, rel_conv)\n",
    "md_log = SVM(md, lr_md, c, 1000, wbc_n, rel_conv)\n",
    "# fit the models...\n",
    "print('Fitting gradient descent:')\n",
    "wbc_gd_loss = gd_log.fit(wbc_X_train, wbc_y_train)\n",
    "print('\\nFitting stochastic gradient descent, batch size = 1:')\n",
    "wbc_sgd_100_loss = sgd_100_log.fit(wbc_X_train, wbc_y_train)\n",
    "print('\\nFitting mirror descent:')\n",
    "wbc_md_loss = md_log.fit(wbc_X_train, wbc_y_train)\n",
    "# print the test accuracies for each model\n",
    "acc = check_accuracy_svm(gd_log, wbc_X_test, wbc_y_test)\n",
    "print(\"\\n\\nGD Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy_svm(sgd_100_log, wbc_X_test, wbc_y_test)\n",
    "print(\"SGD 100 Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy_svm(md_log, wbc_X_test, wbc_y_test)\n",
    "print(\"MD Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "# plot the loss convergences for each model\n",
    "plot_dynamic_svm_losses(wbc_gd_loss, wbc_sgd_100_loss, wbc_md_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Square Root Decay Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our learning rate object\n",
    "lr_gd = ExpDecayRate(0.01, 0.001)\n",
    "lr_sgd = ExpDecayRate(0.01, 0.0001)\n",
    "lr_md = ExpDecayRate(0.001, 0.001)\n",
    "# initialize our descent methods\n",
    "gd = GradientDescent()\n",
    "sgd_100 = GradientDescent()\n",
    "md = MirrorDescent()\n",
    "# initialize logistic regression models\n",
    "gd_log = SVM(gd, lr_gd, c, 2000, wbc_n, rel_conv)\n",
    "sgd_100_log = SVM(sgd_100, lr_sgd, c, 6000, 100, rel_conv)\n",
    "md_log = SVM(md, lr_md, c, 2000, wbc_n, rel_conv)\n",
    "# fit the models...\n",
    "print('Fitting gradient descent:')\n",
    "wbc_gd_loss = gd_log.fit(wbc_X_train, wbc_y_train)\n",
    "print('\\nFitting stochastic gradient descent, batch size = 1:')\n",
    "wbc_sgd_100_loss = sgd_100_log.fit(wbc_X_train, wbc_y_train)\n",
    "print('\\nFitting mirror descent:')\n",
    "wbc_md_loss = md_log.fit(wbc_X_train, wbc_y_train)\n",
    "# print the test accuracies for each model\n",
    "acc = check_accuracy_svm(gd_log, wbc_X_test, wbc_y_test)\n",
    "print(\"\\n\\nGD Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy_svm(sgd_100_log, wbc_X_test, wbc_y_test)\n",
    "print(\"SGD 1 Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy_svm(md_log, wbc_X_test, wbc_y_test)\n",
    "print(\"MD Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "# plot the loss convergences for each model\n",
    "plot_dynamic_svm_losses(wbc_gd_loss, wbc_sgd_100_loss, wbc_md_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see for the WBC data that our algorithms...\n",
    "# Peter \n",
    "{ insert analysis for previous three plots and results here...\n",
    "  \n",
    "  also insert a chart for the timing and iterations to convergence }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wisconsin Breast Cancer Data Set\n",
    "As with Logistic Regression, we begin with observing the performance of the SVM objective across the different algorithms for the different learning rate paradigms investigated, and then across the three data sets. The configuration of the executed cells is the same as with the Logistic Regression, but with SVM model objects instantiated in place of the LogisticRegression ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fixed Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our learning rate object\n",
    "lr = FixedRate(0.001)\n",
    "\n",
    "# initialize our descent methods\n",
    "gd = GradientDescent()\n",
    "sgd_100 = GradientDescent()\n",
    "agd = NesterovAcceleratedDescent()\n",
    "svrg = StochasticVarianceReducedGradientDescent()\n",
    "md = MirrorDescent()\n",
    "# initialize all of the SVM models\n",
    "gd_svm = SVM(gd, lr, c, 20000, wbc_n, rel_conv)\n",
    "sgd_100_svm = SVM(sgd_100, lr, c, 20000, 100, rel_conv)\n",
    "agd_svm = SVM(agd, lr, c, 20000, wbc_n, rel_conv)\n",
    "svrg_svm = SVM(svrg, lr, c, 3000, wbc_n, rel_conv)\n",
    "md_svm = SVM(md, lr, c, 2000, wbc_n, rel_conv)\n",
    "\n",
    "# run fitting for all of the models\n",
    "print('Fitting gradient descent:')\n",
    "wbc_gd_loss = gd_svm.fit(wbc_X_train, wbc_y_train)\n",
    "print('\\nFitting stochastic gradient descent, batch size = 100:')\n",
    "wbc_sgd_100_loss = sgd_100_svm.fit(wbc_X_train, wbc_y_train)\n",
    "print('\\nFitting accelerated gradient descent:')\n",
    "wbc_agd_loss = agd_svm.fit(wbc_X_train, wbc_y_train)\n",
    "print('\\nFitting stochastic variance reduced gradient descent:')\n",
    "wbc_svrg_loss = svrg_svm.fit(wbc_X_train, wbc_y_train)\n",
    "print('\\nFitting mirror descent:')\n",
    "wbc_md_loss = md_svm.fit(wbc_X_train, wbc_y_train)\n",
    "\n",
    "# print test accuracies\n",
    "acc = check_accuracy_svm(gd_svm, wbc_X_test, wbc_y_test)\n",
    "print(\"GD Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy_svm(sgd_100_svm, wbc_X_test, wbc_y_test)\n",
    "print(\"SGD 100 Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy_svm(agd_svm, wbc_X_test, wbc_y_test)\n",
    "print(\"AGD Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy_svm(svrg_svm, wbc_X_test, wbc_y_test)\n",
    "print(\"SVRG Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy_svm(md_svm, wbc_X_test, wbc_y_test)\n",
    "print(\"MD Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "\n",
    "plot_fixed_svm_losses(wbc_gd_loss, wbc_sgd_100_loss, wbc_agd_loss, wbc_svrg_loss, wbc_agd_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exponential Decay Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our learning rate object\n",
    "lr_gd = PolyDecayRate(0.01, 0.0001)\n",
    "lr_sgd = PolyDecayRate(0.01, 0.00001)\n",
    "lr_md = PolyDecayRate(0.001, 0.00001)\n",
    "# initialize our descent methods\n",
    "gd = GradientDescent()\n",
    "sgd_100 = GradientDescent()\n",
    "md = MirrorDescent()\n",
    "# initialize logistic regression models\n",
    "gd_log = SVM(gd, lr_gd, c, 1000, wbc_n, rel_conv)\n",
    "sgd_100_log = SVM(sgd_100, lr_sgd, c, 2000, 100, rel_conv)\n",
    "md_log = SVM(md, lr_md, c, 1000, wbc_n, rel_conv)\n",
    "# fit the models...\n",
    "print('Fitting gradient descent:')\n",
    "wbc_gd_loss = gd_log.fit(wbc_X_train, wbc_y_train)\n",
    "print('\\nFitting stochastic gradient descent, batch size = 1:')\n",
    "wbc_sgd_100_loss = sgd_100_log.fit(wbc_X_train, wbc_y_train)\n",
    "print('\\nFitting mirror descent:')\n",
    "wbc_md_loss = md_log.fit(wbc_X_train, wbc_y_train)\n",
    "# print the test accuracies for each model\n",
    "acc = check_accuracy_svm(gd_log, wbc_X_test, wbc_y_test)\n",
    "print(\"\\n\\nGD Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy_svm(sgd_100_log, wbc_X_test, wbc_y_test)\n",
    "print(\"SGD 100 Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy_svm(md_log, wbc_X_test, wbc_y_test)\n",
    "print(\"MD Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "# plot the loss convergences for each model\n",
    "plot_dynamic_svm_losses(wbc_gd_loss, wbc_sgd_100_loss, wbc_md_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Square Root Decay Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our learning rate object\n",
    "lr_gd = ExpDecayRate(0.01, 0.001)\n",
    "lr_sgd = ExpDecayRate(0.01, 0.0001)\n",
    "lr_md = ExpDecayRate(0.001, 0.001)\n",
    "# initialize our descent methods\n",
    "gd = GradientDescent()\n",
    "sgd_100 = GradientDescent()\n",
    "md = MirrorDescent()\n",
    "# initialize logistic regression models\n",
    "gd_log = SVM(gd, lr_gd, c, 2000, wbc_n, rel_conv)\n",
    "sgd_100_log = SVM(sgd_100, lr_sgd, c, 6000, 100, rel_conv)\n",
    "md_log = SVM(md, lr_md, c, 2000, wbc_n, rel_conv)\n",
    "# fit the models...\n",
    "print('Fitting gradient descent:')\n",
    "wbc_gd_loss = gd_log.fit(wbc_X_train, wbc_y_train)\n",
    "print('\\nFitting stochastic gradient descent, batch size = 1:')\n",
    "wbc_sgd_100_loss = sgd_100_log.fit(wbc_X_train, wbc_y_train)\n",
    "print('\\nFitting mirror descent:')\n",
    "wbc_md_loss = md_log.fit(wbc_X_train, wbc_y_train)\n",
    "# print the test accuracies for each model\n",
    "acc = check_accuracy_svm(gd_log, wbc_X_test, wbc_y_test)\n",
    "print(\"\\n\\nGD Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy_svm(sgd_100_log, wbc_X_test, wbc_y_test)\n",
    "print(\"SGD 1 Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "acc = check_accuracy_svm(md_log, wbc_X_test, wbc_y_test)\n",
    "print(\"MD Accuracy: {0:.2f}%\".format(acc * 100))\n",
    "# plot the loss convergences for each model\n",
    "plot_dynamic_svm_losses(wbc_gd_loss, wbc_sgd_100_loss, wbc_md_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see for the WBC data that our algorithms...\n",
    "# Peter \n",
    "{ insert analysis for previous three plots and results here...\n",
    "  \n",
    "  also insert a chart for the timing and iterations to convergence }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
